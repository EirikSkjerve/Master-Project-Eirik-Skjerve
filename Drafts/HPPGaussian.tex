\documentclass[12 pt]{article}        	%sets the font to 12 pt and says this is an article (as opposed to book or other documents)
\usepackage{amsfonts, amssymb}					% packages to get the fonts, symbols used in most math
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{amsthm}
\usepackage{mathtools, nccmath}
\DeclarePairedDelimiter{\nint}\lfloor\rceil

\usepackage[integrals]{wasysym}
\usepackage{latexsym}
\DeclareRobustCommand{\VAN}[3]{#2}
\usepackage{amssymb} 
\usepackage{cite}

%\usepackage{setspace}               		% Together with \doublespacing below allows for doublespacing of the document

\oddsidemargin=-0.5cm                 	% These three commands create the margins required for class
\setlength{\textwidth}{6.5in}         	%
\addtolength{\voffset}{-20pt}        		%
\addtolength{\headsep}{25pt}

\newcommand{\PP}[2][]{\mathcal{P}_{#1}(\mat{#2})}
\newcommand{\mat}[1]{\mathit{#1}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\GLnR}{\mathcal{GL}_{n}(\mathbb{R})}
\newcommand{\normdist}[2]{\mathcal{N}(#1, #2^2)}
\newcommand{\dgdist}{\mathcal{D}_{2\bb{Z}+c, \sigma}}
\newcommand{\bb}[1]{\mathbb{#1}}

\begin{document}

\title{Proof for why HPP does not work if samples are normally distributed}
\author{Eirik D. Skjerve}
\maketitle

\section{HPP against normally distributed samples}
In the following, we see what happens to the computations the \textit{Learning a parallelepiped} attack is based on if we replace the uniform distribution by a normal distribution.
The key component and assumption of the \textit{Learning a parallelepiped} attack is that the provided samples are distributed uniformly over $\PP{V}$.
One runs into trouble if the sampled vectors are on the form $\vec{v} = \vec{x} \mat{V}$ where $\vec{x}$ follows a normal distribution, i.e. $x_i \sim \normdist{\mu}{\sigma}$.
Although one might be able to approximate the covariance matrix $\mat{V}^t \mat{V}$ and transform the hidden parallelepiped to a hidden hypercube, 
one can not do a gradient descent based on the fourth moment given such samples using the method from the original attack \cite{NR09}. We will show that if samples follow a normal distribution, 
the fourth moment of $\PP{C}$ over $\vec{w}$ on the unit circle is constant, and therefore a gradient descent can not reveal any information about the secret key $\mat{V}$.

\paragraph{Adapting the definition of $\PP{V}$}
Recall that $\PP{V}$ is defined as $\{\sum_{i=1}^n x_i \vec{v}_i : x_i \in [-1, 1]\}$ where $\vec{v}_i$ are rows of $\mat{V}$ and $x_i$ is uniformly distributed over $[-1, 1]$ 
(generally one can take another interval than $[-1, 1]$ and do appropriate scaling).
Firstly, the normal distribution $\normdist{\mu}{\sigma}$ is defined over $(- \infty, \infty)$, so it does not make sense to talk about samples "normally distributed over $\PP{V}$" without tweaking any definitions.
Therefore, let $[- \eta, \eta]$ be a finite interval on which to consider a truncated normal distribution $\mathcal{N}_{\eta}(\mu, \sigma^2)$ such that $\int_{-\eta}^{\eta} f_X(x) dx = 1 - \delta$ for some negligibly small $\delta$
where $f_X(x)$ is the probability density function of $\normdist{\mu}{\sigma}$, given by $f_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x- \mu)^2}{2 \sigma^2}}$.
Now we consider $\PP[\eta]{V} = \{\sum_{i=1}^n x_i \vec{v}_i : x_i \in [-\eta, \eta]\}$ and proceed as in the original HPP with $\PP[\eta]{V}$ instead of $\PP{V}$.

\paragraph{Approximating $\mat{V}^t \mat{V}$}
Let $\mat{V} \in \GLnR$. Let $\vec{v}$ be chosen from a truncated normal distribution $\mathcal{N}_{\eta}(0, \sigma^2)$ over $\PP[\eta]{V}$.
Then $\lim_{\eta\to\infty}$ $\bb{E}[\vec{v}^t\vec{v}] = \mat{V}^t \mat{V} \cdot \sigma^2$.

\begin{proof}
    Let samples be on the form $\vec{v} = \vec{x}\mat{V}$, where $\vec{x}$ is a row vector where each element $x_i \sim \mathcal{N}_{\eta}(0, \sigma^2)$.
    Then $\vec{v}^t\vec{v} = (\vec{x} \mat{V})^t (\vec{x} \mat{V}) = (\mat{V}^t \vec{x}^t)(\vec{x} \mat{V}) = \mat{V}^t \vec{x}^t \vec{x} \mat{V}$. Considering $\bb{E}[\vec{x}^t \vec{x}]$, we see that for $i \neq j$, 
$\bb{E}[x_i x_j] = \bb{E}[x_i] \bb{E}[x_j] = 0 \cdot 0 = 0$ due to independent random variables.
For $i=j$, $\lim_{\eta\to\infty} \bb{E}[x_i^2] = \bb{V}[x_i] = \sigma^2$ since $\bb{V}[x_i] = \bb{E}[x_i^2] - \bb{E}[x_i]^2 = \bb{E}[x_i^2] - 0 = \sigma ^2$.
Therefore, $\lim_{\eta\to\infty} \bb{E}[\vec{x}^t \vec{x}] = \mat{I}_n \cdot \sigma^2$, i.e. the matrix with $\sigma ^2$ on the diagonal.
Consequently, $\lim_{\eta\to\infty} \vec{v}^t \vec{v} = \mat{V}^t \bb{E}[\vec{x}^t\vec{x}] \mat{V} = \mat{V}^t (\mat{I}_n \cdot \sigma^2) \mat{V} = (\mat{V}^t \mat{V}) \cdot \sigma ^2$ 
and conversely $\lim_{\eta\to\infty} \mat{V}^t \mat{V} = (\vec{v}^t \vec{v})/ \sigma^2$.
\end{proof}

This means that we can in theory approximate the covariance matrix $\mat{V}^t \mat{V}$ by averaging over $\vec{v}^t \vec{v}$ and dividing by $\sigma ^2$. 
However, it is not immediately clear if one needs more samples for this approximation than in the original attack due to the difference in distributions.

\paragraph{Hypercube transformation}
Assume now that we know $\mat{V}^t \mat{V}$. Consider instead of $\PP{V}$, $\PP[\eta]{V}$.
Then by following part 1 of \textbf{Lemma 2} and its proof from \cite{NR09} we can transform our hidden parallelepiped $\PP[\eta]{V}$ into $\PP[\eta]{C}$, a hidden hypercube,
since this does not depend on the distribution of the samples - it only assumes one knows $\mat{V}^t\mat{V}$.
For completeness, by adapting the second part of \textbf{Lemma 2} to our case: 
\begin{proof}
    Let $\vec{v} = \vec{x}\mat{V}$ where $\vec{x}$ is normally distributed according to $\mathcal{N}_{\eta}(0, \sigma^2)$.
    Then samples $\vec{v}$ are distributed according to $\mathcal{N}_{\eta}(0, \sigma^2)$ over $\PP[\eta]{V}$.
    It then follows that $\vec{v}\mat{L} = \vec{x}\mat{V}\mat{L} = \vec{x}\mat{C}$ has a truncated uniform distribution over $\PP[\eta]{C}$.
\end{proof}
Thus, we should be able to map our normally distributed samples from the hidden parallelepiped to the hidden hypercube.

\paragraph{Learning a hypercube}
It is clear that samples uniformly over $\PP[\eta]{C}$ centered at the origin form a hypersphere for which any orthogonal rotation leaves the sphere similar in shape. 
As a consequence, the fourth moment of $\PP[\eta]{C}$ is constant over the unit circle. \\ 

Analogous to \cite{NR09} we compute the 2nd and 4th moment of $\PP[\eta]{V}$ over a vector $\vec{w} \in \bb{R}^n$.
The \textit{k}-th moment of $\PP[\eta]{V}$ over a vector $\vec{w}$ is defined as $mom_{\mat{V}, k} = \bb{E}[\langle \vec{u}, \vec{w} \rangle ^k]$ where $\vec{u} = \sum_{i=1}^{n} x_i \vec{v}_i$ and $x_i \sim \mathcal{N}_{\eta}(0, \sigma^2)$.
First we consider $\langle \vec{u}, \vec{w} \rangle = \langle \sum_{i=1}^{n} x_i\vec{v}_i, \vec{w}\rangle = \sum_{i=1}^{n}x_i \langle\vec{v}_i, \vec{w} \rangle$.
Then for $k=2$, $\bb{E}[(\sum_{i=1}^{n}x_i \langle\vec{v}_i, \vec{w} \rangle)^2] = \bb{E}[\sum_{i=1}^{n} \sum_{j=1}^{n} x_i x_j \langle \vec{v}_i, \vec{w} \rangle \langle \vec{v}_j \vec{w} \rangle]$.
Due to independent random variables, $\bb{E}[x_i x_j] = 0$ when $i \neq j$, so we have $ \sum_{i=1}^{n}\bb{E}[x_i^2]\langle \vec{v}_i, \vec{w} \rangle^2$ 
$= \sigma^2 \sum_{i=1}^{n}\langle \vec{v}_i, \vec{w} \rangle^2$ for sufficiently large $\eta$ due to the well known result that $\bb{E}[x^2] = \sigma^2$ for $x \sim \mathcal{N}(0, \sigma^2)$.
Thus, we end up with:
\begin{equation}
mom_{\mat{V}, 2}(\vec{w}) = \sigma^2 \vec{w}\mat{V}^t\mat{V}\vec{w}^t \\
\end{equation}
We observe that if $\mat{V} \in \mathcal{O}(\bb{R})$, $mom_{\mat{V}, 2}(\vec{w}) = \sigma^2 \left \Vert \vec{w} \right \Vert^2$ \\

For $k=4$:
\[
    \bb{E}[(\sum_{i=1}^{n} x_i \langle \vec{v}_i, \vec{w} \rangle)^4] = \bb{E}[\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n}\sum_{l=1}^{n}x_i x_j x_k x_l \langle \vec{v}_i, \vec{w} \rangle \langle \vec{v}_j, \vec{w} \rangle \langle \vec{v}_k, \vec{w} \rangle \langle \vec{v}_l, \vec{w} \rangle]
\]
We consider three cases for the indices $i, j, k,$ and $l$:
\begin{enumerate}
    \item \textbf{None equal}: if $i, j, k,$ and $l$ are different, the expression equals 0 due to independent random variables.
    \item \textbf{All equal}: if $i = j = k = l$, then we have $\sum_{1=1}^{n} \bb{E}[x_i ^4] \langle \vec{v}_i, \vec{w} \rangle^4$. 
        A well known result for the normal distribution $\normdist{0}{\sigma}$ is that $\bb{E}[x^4] = 3 \sigma^4$.
    \item \textbf{Pairwise equal}: if either 
        \begin{itemize}
            \item $i=j \neq k=l$
            \item $i=l \neq j=k$
            \item $i=k \neq j=l$
        \end{itemize}
        we have the following:
        \[
            \sum_{i\neq j} \bb{E}[x_i^2x_j^2] \langle \vec{v}_i, \vec{w} \rangle ^2 \langle \vec{v}_j, \vec{w} \rangle^2
        \]
        Since $\bb{E}[x_i^2x_j^2] = \bb{E}[x_i^2]\bb{E}[x_j^2] = \sigma^4$ due to independent random variables, we have
        \[
            \sigma^4 \sum_{i\neq j} \langle \vec{v}_i, \vec{w} \rangle ^2 \langle \vec{v}_j, \vec{w} \rangle^2
        \]
\end{enumerate}
Putting together the expressions above we have
\[mom_{\mat{V}, 4}(\vec{w}) = 3 \sigma^4 \sum_{i=1}^{n} \langle \vec{v}_i, \vec{w}\rangle^4 + 3(\sigma^4 \sum_{i \neq j} \langle \vec{v}_i, \vec{w}\rangle^2 \langle \vec{v}_j, \vec{w}\rangle^2)\] 
since there are three cases where indices pair up two and two. The final result becomes:
\begin{equation}
    mom_{\mat{V}, 4}(\vec{w}) = 3 \sigma^4 (\sum_{i=1}^{n} \langle \vec{v}_i, \vec{w}\rangle^4 + \sum_{i \neq j} \langle \vec{v}_i, \vec{w}\rangle^2 \langle \vec{v}_j, \vec{w}\rangle^2)
\end{equation}

Claim: If $\mat{V} \in \mathcal{O}(\bb{R})$, and $\vec{w}$ is on the unit sphere, $mom_{\mat{V}, 4}(\vec{w})$ is constant. 

\begin{proof}
This can be shown by rewriting (3.2) as  
\[ mom_{\mat{V}, 4}(\vec{w}) = 3\sigma^4(\sum_{i=1}^n \langle \vec{v}_i, \vec{w}\rangle^4 + \sum_{i=1}^n \langle \vec{v}_i, \vec{w} \rangle^2 \sum_{j=1}^n \langle \vec{v}_j, \vec{w}\rangle^2 - \sum_{i=1}^n\langle \vec{v}_i, \vec{w}\rangle^4 )\]
\[ mom_{\mat{V}, 4}(\vec{w}) = 3\sigma^4(\sum_{i=1}^n \langle \vec{v}_i, \vec{w}\rangle^2 \sum_{j=1}^n \langle \vec{v}_j, \vec{w}\rangle^2) = 3\sigma^4(\sigma^2\left \Vert \vec{w} \right \Vert^2)^2 = 3\sigma^8\]
because $mom_{\mat{V}, 2}(\vec{w}) = \sum_{i=1}^n \langle \vec{v}_i, \vec{w}\rangle^2 = \sigma^2 \left \Vert \vec{w} \right \Vert^2$ when $\mat{V} \in \mathcal{O}(\bb{R})$ and $\left \Vert \vec{w} \right \Vert^2 = 1$ when $\vec{w}$ lies on the unit sphere.
\end{proof}

In conclusion, if samples over the secret parallelepiped $\PP[\eta]{V}$ follow a continuous normal distribution, a gradient descent based on the fourth moment described in \cite{NR09}
is impossible because the fourth moment is constant over the unit sphere of $\bb{R}^n$.

\paragraph{The discrete Gaussian distribution}
We now do the same computations considering the discrete Gaussian distribution as described in \cite{HawkSpec24}. 
It turns out that for the parameters used in Hawk, this distribution behaves like its corresponding Normal distribution.\\ 
Consider the discrete Gaussian distribution as described in \cite{HawkSpec24}. \\
If $X \sim \mathcal{D}_{2 \bb{Z} + c, \sigma}$, we have Supp$(X) = 2 \bb{Z} + c$ where $c \in \{0, 1\}$, and Pr$[X = x] = \frac{\rho_{\sigma}(x)}{\sum_{y \in 2 \bb{Z} + c} \rho_{\sigma}(y)}$
where $\rho_{\sigma}(x) = e^{-\frac{x^2}{2 \sigma^2}}$. For $c \in \{0, 1 \}$, $\bb{E}[X] = 0$ and $\bb{V}[X] = \sigma^2$ for appropriate choices of $\sigma$.

\[\cdots Show \ definitions \ and \ stuff \ here \cdots\]

Then by computational results, as $\eta$ grows large enough, $\bb{E}[x^2] = \sum_{-\eta}^{\eta} x^2 f_X(x)$ and $\bb{E}[x^4] = \sum_{-\eta}^{\eta} x^4 f_X(x)$ approaches $\sigma^2$ and $3 \sigma^4$ respectively. 

\[\cdots Show \ table \ of \ computations \ here \cdots\]
% As a countermeasure against these types of attacks, it was proposed in \cite{GPV07} to not use the uniform distribution when creating signatures. Rather, by sampling from a Discrete Gaussian distribution
% (a discrete analogue to the Normal distribution), signatures do not "leak" any information about the secret parallelepiped, and the geometry of the key remains concealed.

\bibliographystyle{plain}  % Choose your desired style (plain, IEEE, etc.)
\bibliography{refs}        % This references the refs.bib file
\end{document}

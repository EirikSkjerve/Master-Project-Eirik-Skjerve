
\documentclass[12 pt]{article}        	%sets the font to 12 pt and says this is an article (as opposed to book or other documents)
\usepackage{amsfonts, amssymb}					% packages to get the fonts, symbols used in most math
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{amsthm}
\usepackage{mathtools, nccmath}
\DeclarePairedDelimiter{\nint}\lfloor\rceil

\usepackage[integrals]{wasysym}
\usepackage{latexsym}
\DeclareRobustCommand{\VAN}[3]{#2}
\usepackage{amssymb} 
\usepackage{cite}

%\usepackage{setspace}               		% Together with \doublespacing below allows for doublespacing of the document

\oddsidemargin=-0.5cm                 	% These three commands create the margins required for class
\setlength{\textwidth}{6.5in}         	%
\addtolength{\voffset}{-20pt}        		%
\addtolength{\headsep}{25pt}

\newcommand{\PP}[2][]{\mathcal{P}_{#1}(\mat{#2})}
\newcommand{\mat}[1]{\mathit{#1}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\GLnR}{\mathcal{GL}_{n}(\mathbb{R})}
\newcommand{\normdist}[2]{\mathcal{N}(#1, #2^2)}
\newcommand{\dgdist}{\mathcal{D}_{2\bb{Z}+c, \sigma}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\dgd}{\mathcal{D}}
\newcommand{\dgdi}{\widehat{\mathcal{D}}}
\renewcommand{\dot}[2]{\langle \vec{#1}, \vec{#2} \rangle}
\begin{document}

\title{}
\author{Eirik D. Skjerve}
\maketitle
\section{\textit{Learning a parallelepiped} attack against the Discrete Gaussian Distribution in Hawk}
We consider the Discrete Gaussian Distribution as described in \cite{HawkSpec24}. Available we have an implementation of the sampling method used in the Hawk scheme,
using precomputed cumulative distribution tables to emulate samples from the theoretical distribution. Using this implementation we can sample an arbitrary number of sample points, which we will use in this attack. \\ \hfill \break

Let $\dgd$ denote the theoretical discrete Gaussian distribution as described in Hawk spec-paper \cite{HawkSpec24}.
Let $\dgdi$ denote the distribution of samples from the practical implementation using tables,
and let $\mu, \sigma ^2$ be the expectation and variance respectively of $\dgdi$. We can estimate these values simply by collection many samples from the implementation.
Say we sample $n$ points as $X = \{x_1, x_2,..., x_n\}$. First we estimate $\hat{\mu}$ = $\frac{1}{n}\sum_{i=1}^{n} x_i$ and $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \hat{\mu})^2$.
It is possible that we should just use $\hat{\mu} = 0$. Then we normalize the samples by computing 
$Z = \{z_1, z_2,..., z_n\} = \{\frac{x_1}{\hat{\sigma}}, \frac{x_2}{\hat{\sigma}},..., \frac{x_n}{\hat{\sigma}}\} $ such that $\bb{V}(z_i) = 1$. This makes further computations easier to work with. \\ \hfill \break

Now, like in the original \textit{Learning a parallelepiped} attack, let $\mat{V}$ be a secret $n \times n$ orthonormal matrix (this is after transforming hidden parallelepiped to hidden hypercube),
and that we observe many signatures on the form $\vec{v} = \mat{V}\vec{z}$ where $z_i \sim \dgdi$ and $\bb{V}(z_i) = 1$.

We compute the 2nd and 4th moment of $\PP{V}$ over a vector $\vec{w} \in \bb{R}^n$ as $\bb{E}[\langle \vec{u}, \vec{w} \rangle ^k]$ for $k=2,4$ where $\vec{u} = \sum_{i=1}^{n}z_i \vec{v}_i.$ 
\begin{itemize}
    \item $\bb{E}[\langle \vec{u}, \vec{w} \rangle ^2] = \bb{E}[(\sum_{i=1}^{n} z_i \langle \vec{v}_i, \vec{w} \rangle )^2] = \sum_{i=1}^{n}\bb{E}[z_i^2]\langle \vec{v}_i, \vec{w} \rangle^2 = \Vert \vec{w} \Vert ^2$
    \item $\bb{E}[\dot{v}{w} ^4] = \sum_{i=1}^{n}\bb{E}[z_i ^4] \langle \vec{v}_i, \vec{w} \rangle  + 3\sum_{i \neq j} \bb{E}[z_i^2 z_j^2] \langle \vec{v}_i, \vec{w} \rangle ^2 \langle \vec{v}_j \vec{w} \rangle^2$ \\
        $= \mu_4 \sum_{i=1}^{n} \langle \vec{v}_i, \vec{w} \rangle + 3\sum_{i \neq j} \langle \vec{v}_i, \vec{w} \rangle ^2 \langle \vec{v}_j \vec{w} \rangle^2$
        due to independent variables and $\bb{V}[z_i] = 1$, where $\mu_4 = \bb{E}[z_i ^4]$.
\end{itemize}
Since $\mat{V}$ is orthonormal and if $\vec{w}$ is on the unit sphere, we have 
\[\bb{E}[\dot{v}{w} ^4] = \mu_4 \sum_{i=1}^{n} \langle \vec{v}_i, \vec{w} \rangle ^4 + 3(\sum_{i=1}^{n} \langle \vec{v}_i, \vec{w} \rangle^2 \sum_{j=1}^{n} \langle \vec{v}_j, \vec{w} \rangle^2 - \sum_{i=1}^{n} \langle \vec{v}_i, \vec{w} \rangle ^4)\]
\[= (\mu_4 - 3)\sum_{i=1}^{n}\langle \vec{v}_i, \vec{w} \rangle^4 + 3 \Vert \vec{w} \Vert ^4\]

By collecting and analyzing samples from $\dgdi$ and estimate $\mu_4$ we can do gradient descent as in the original attack if $\lvert \mu_4 - 3 \rvert \neq 0$.
If $\mu_4 - 3 > 0$ we need to minimize the term $(\mu_4 - 3) \sum_{i=1}^{n} \langle \vec{v}_i, \vec{w} \rangle ^4$. If $\mu_4 - 3 < 0$, we need to maximize the same term to minimize the total expression for the 4th moment.
\bibliographystyle{plain}  % Choose your desired style (plain, IEEE, etc.)
\bibliography{refs}        % This references the refs.bib file
\end{document}

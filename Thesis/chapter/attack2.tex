\section{Alternative method}

We have established that the direct method of observing $\vec{w} = \mat{B}^{-1} \vec{x}$ will not lead to a successful attack.
We now try an alternative method.

Consider in Hawk signature generation as described in section 3.
The secret vector $\vec{x}$ is sampled from $\dgd_{2\bb{Z}^{2n} + \vec{t}, \sigma}$.
Since $\vec{t}$ is defined as $\mat{B}\vec{h}\mod 2$ (entrywise reduction$\mod 2$), we can rewrite $\vec{t}$ as $\vec{t} = \mat{B} \vec{h} + 2 \cdot \vec{d}$, 
for an integer vector $\vec{d}$ such that adding $2 \cdot \vec{d}$ is equivalent to reducing $\mod 2$.

Now, we also consider $\vec{x}$, which can be written as $\vec{x} = \vec{t} + 2 \cdot \vec{y}$ for some integer vector $\vec{y}$.
Then we have the following: \\ 
$\vec{w} = \mat{B}^{-1} \vec{x} = \mat{B}^{-1}(\vec{t} + 2 \cdot \vec{y}) =$ \\
$\mat{B}^{-1}(\mat{B} \vec{h} + 2 \cdot \vec{d} + 2 \cdot \vec{y}) =$ \\
$\vec{h} + 2\cdot \mat{B}^{-1}(\vec{d} + \vec{y}) $\\ 
$\implies \mathlarger{ \frac{\vec{w} - \vec{h}}{2}} = \mat{B}^{-1}(\vec{d} + \vec{y})$

Since both $\vec{w}$ and $\vec{h}$ is available to an attacker, we analyze the distribution of $\vec{x}_1 = (\vec{d} + \vec{y})$.
% If the distribution of $\vec{x}_1$ is far enough away from a perfect Gaussian distribution (which has kurtosis 3), then observing many $\vec{w}_1 = \mathlarger{ \frac{\vec{w} - \vec{h}}{2}} = \mat{B}^{-1} \vec{x}_1$
% might reveal the columns of $\mat{B}^{-1}$.
Note that the distribution of $\vec{y}$ in this case will be related to the discrete Gaussian distribution $\dgd_{\bb{Z}^{2n}, \sigma}$, which we have already analyzed.

The next step is then to analyze the distribution that $\vec{d}$ follows. Note that the vector $\vec{h}$, which is originally a hash digest of length $n / 4$ of the message $\vec{m}$ and a salt $\mathsf{salt}$, is converted into
a binary polynomial of length $2n$. We assume $\vec{h}$ is uniformly distributed.

Firstly, we need to estimate (most probably experimentally by samples) the distribution that $\vec{d}$ follows.
We do this experimentally by running the signature generation algorithm, modified in this case to return also $\vec{d} = \mathlarger{\frac{\vec{t}-\mat{B} \vec{h}}{2}}$.
Note that $\vec{d}$ is not available information for an attacker in real life, and so we only use it now to do experiments on the distribution $\vec{d}$.
From this we have $m$ observed sample points. From such observed points $d_j$ we compute 
$\mu = \mathlarger{\frac{1}{m} \sum_{j = 0}^{m}} d_j$ and similarly for its variance $\sigma^2 = \mathlarger{\frac{1}{m} \sum_{j = 0}^{m}} (d_j - \mu)^2$

% The question is, is its distribution possible to estimate, and how many samples do we actually need?
% By simple observations, $\vec{d}$ has an easily observable distribution (one needs few samples) for one fixed key. This is tested by measuring $\mu$ and $\sigma^2$ of independently generated samples with the
% same private key. The results show very close results for $\mu$ and $\sigma^2$ for the same key, even with few samples of $\vec{d}$ (say 100,000).

% For a fixed secret key, the expectation and variance for $\vec{d}$
For different secret keys, the values for $\mu$ and $\sigma^2$ vary quite a lot.
Therefore it might be challenging to estimate.
The hope now is that if we can estimate the distribution of $\vec{y}$, and then observe public signature samples on the form $\vec{w}_1 = \mat{B}^{-1} \vec{x}_1$, we can easily deduce the distribution of $\vec{d}$.

Now on to the distribution of $\vec{y}$. 
Trivially, based on the tables and definitions, $Pr(y = k) = Pr(x=2k) + Pr(x = 2k + 1)$ since $\vec{y} = \mathlarger{\frac{\vec{x} - \vec{t}}{2}}$.
% , and thus $\vec{y}$ and $\vec{t}$, and consequently $\vec{y}$ and $\vec{d}$ are independent.
From this we have that 
\begin{itemize}
    \item $\bb{E}[y] = -0.25$
    \item $\bb{E}[y^2] = 1.1451$ 
    \item $\bb{E}[y^4] = 3.918137$.
\end{itemize}
Now, assume now we observe many signatures on the form $\frac{\vec{w} - \vec{h}}{2} = \mat{B}^{-1}(\vec{d} + \vec{y})$.
After computing $\mat{L}^{T}$ and computing $\mat{L}^T \frac{\vec{w} - \vec{h}}{2} = \mat{L}^T \mat{B}^{-1}(\vec{d} + \vec{y}) = \mat{C} (\vec{d} + \vec{y})$.
Now we can measure the expectation $\bb{E}[\vec{d}_i + \vec{y}_i]$ and variance $\bb{E}[(\vec{d}_i + \vec{y}_i )^2]$ by averaging over all available points since $\mat{C}$ preserves the distribution.

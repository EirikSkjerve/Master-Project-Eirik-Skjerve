\newtheorem{hpp_problem_hawk}{Problem}

\chapter{Cryptanalysis of Hawk}
\section{Overview}
In this chapter we introduce our cryptanalysis of Hawk by adapting the original HPP attack to the Hawk setting.
As previously shown, a continuous normal distribution is immune against such an attack due to the constant 4-th moment. In Hawk, however, the distribution of entries in $\vec{x}$ is discrete, not continuous.
We will show in section 4.2 that the Discrete Gaussian Distribution based on tables and the algorithm \ref{sample} is indeed not identical to a continuous normal distribution, and that an attack akin to the original HPP may still be applicable.

In section 4.3, we explain the procedure to transform the hidden parallelepiped to a hidden hypercube, and show why this is a trivial task for Hawk signatures.
Section 4.4 will cover the main part of the attack, namely the gradient descent, which will in many ways be similar to the original attack.
Lastly, section 4.5 will present the complete practical method and give details about its implementation.

\section{Redefining the problem and solution}

We now restate the Hidden Parallelepiped Problem in the Hawk setting.
Recall in HPP that one observes row vectors $\vec{v} = \vec{x} \mat{V}$. In the Hawk setting, the observed signature is $\vec{w} = \mat{B}^{-1} \vec{x}$ since we have moved to column-notation for vectors. 
The problem to solve is formulated in Problem \ref{HPP problem Hawk}

\begin{hpp_problem_hawk}\label{HPP problem Hawk}
Given signature samples on the form $\vec{w} = \mat{B}^{-1} \vec{x}$ where only $\vec{w}$ is known, $\mat{B}$ is a Hawk private key and $\vec{x}$ is distributed according to the discrete Gaussian Distribution $\dgd_{2\bb{Z}^{2n} + \vec{t}, \sigma}$,
recover columns of $\mat{B}$.
\end{hpp_problem_hawk}
The general steps in Hawk setting will be similar to that of the original HPP:
\begin{itemize}
    \item Compute Covariance Matrix $(\mat{B}^{-1})^T (\mat{B}^{-1})$
    \item Transform the samples $\vec{w} \in \PP{\mat{B}^{-1}}$ to $\vec{c} \in \PP{C}$ where $\mat{C}$ is orthonormal
    \item Deduce columns of $\mat{C}$ doing gradient descent of minimizing the 4th moment of one-dimensional projections of $\PP{C}$
\end{itemize}

Note that in this section we denote by $\vec{u}$ a vector on the unit sphere $\bb{R}^{2n}$ instead of $\vec{w}$ as in the previous section, to avoid confusion with the Hawk notation for $\vec{w} = \mat{B}^{-1} \vec{x}$.
Also note that henceforth we will consider only $\vec{w} = \mat{B}^{-1} \vec{x}$ as a signature instead of $\vec{s} = \frac{1}{2}(\vec{h} - \vec{w})$ since $\vec{w}$ is easily recoverable given $\vec{s}$ and message $\vec{m}$.
Lastly, we consider $\mat{B}$ as $\mathsf{rot}(\mat{B})$ and thus work with matrices in $\bb{Q}^{2n \times 2n}$ and column vectors in $\bb{Q}^{2n}$.

\subsection{Covariance matrix and hypercube transformation}
In the original HPP attack one has to estimate the matrix $\mat{G} \approx \mat{V} ^t \mat{V}$.
For Hawk, the signatures are on the form $\vec{w} = \mat{B}^{-1} \vec{x}$ which means we would need to compute $\mat{G} = \mat{B}^{-1} \mat{B}^{-T}$ 
(recall the HPP paper uses row vectors while Hawk use columns vectors).
However, the public key $\mat{Q} = \mat{B}^T \mat{B}$, enables us to skip this step.
In the original attack one takes Cholesky decomposition of the inverse of the covariance matrix such that $\mat{G}^{-1} = \mat{L}\mat{L}^T$. 
For $\mat{G} = \mat{B}^{-1} \mat{B}^{-T}$, the inverse,
$\mat{G}^{-1} = \mat{B}^T \mat{B} = \mat{Q}$. Therefore, we can simply take the Cholesky decomposition of $\mat{Q}$ to get $\mat{L} \text{ such that } \mat{Q} = \mat{L} \mat{L}^T$.
By multiplying our samples $\vec{w}$ by $\mat{L}^T$ on the left, we have transformed our samples to the hidden hypercube as in the original attack. \\
By taking $\mat{C} =  \mat{L}^T\mat{B}^{-1}$, we have that 
\[\mat{C}^T \mat{C} = (\mat{L}^T \mat{B}^{-1})^T (\mat{L}^T \mat{B}^{-1}) = \mat{B}^{-T} \mat{L} \mat{L}^T \mat{B}^{-1} = \mat{B}^{-T} \mat{Q} \mat{B}^{-1} = \mat{B}^{-T} \mat{B}^{T} \mat{B} \mat{B}^{-1} = \mat{I}_n\]
and
\[\mat{C}\mat{C}^T = (\mat{L}^T \mat{B}^{-1})(\mat{L}^T \mat{B}^{-1})^T = \mat{L}^T \mat{B}^{-1} \mat{B}^{-T} \mat{L} =  \mat{L}^T \mat{Q}^{-1} \mat{L} = \mat{L}^T (\mat{L} \mat{L}^T)^{-1} \mat{L} = \mat{L}^T \mat{L}^{-T} \mat{L}^{-1} \mat{L} = \mat{I}_n\]
Thus $\mat{C}$ is an orthonormal matrix.
Since $\vec{w}$ is distributed according to $\dgdi$ over $\PP{\mat{B}^{-1}}$, by taking 
$\vec{c} = \mat{L}^{T} \vec{w}$ we have $\vec{c} = \mat{L}^{T} \mat{B}^{-1} \vec{x} = \mat{C} \vec{x}$, $\vec{c}$ is distributed according to $\dgdi$ over $\PP{C}$.
Lastly, we also want to normalize the distribution of entries in $\vec{x}$ to have variance 1. Therefore we consider instead
$\vec{c} = \mat{C} \vec{z} = \mathlarger{\frac{\mat{C}\vec{x}}{\sigma}}$ where $\sigma$ is the std.dev. of $\dgd_{2\bb{Z} + \vec{t}, \sigma}$.
% , by dividing our samples $\vec{w}$ by $\sigma$ in addition to multiplying them with $\mat{L}^{-1}$.
We summarize this procedure in the following algorithm:

\begin{algorithm}
\caption{Hawk Hypercube Transformation}
\begin{algorithmic}[1]
% \Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
    \Require{Samples $\vec{w} = \mat{B}^{-1} \vec{x}$ and public key $\mat{Q}$}
    \State Compute $\mat{L}$ s.t. $\mat{Q} = \mat{L}\mat{L}^T$ \Comment by Cholesky decomposition
    \State Compute $\vec{c} = \mat{L}^T \mathlarger{\frac{\vec{w}}{\sigma}}$ \Comment entrywise division by $\sigma$
    \State \Return{$\vec{c}$ and $\mat{L}^{-T}$}
\end{algorithmic}
\end{algorithm}

\subsection{Moments and gradient search}
Assume now observed signatures on the form $\vec{c} = \mat{C} \vec{z}$ where $\mat{C}$ is orthonormal, and the normalized distribution entries in $\vec{z}$ follows have variance 1. 
By rewriting the terms from section 3.3.4 for this distribution, we have that
\[mom_{4, \mat{C}} (\vec{u}) = 3 \lVert \vec{u} \rVert ^4 + (\mu_4 - 3) \sum_{i=1}^{n} \langle c_i, \vec{u} \rangle^4 \]
and
\[\nabla mom_{4, \mat{C}} (\vec{u}) = 12 \lVert \vec{u} \rVert^2 \vec{u} + 4(\mu_4 - 3) \mathlarger{\sum_{i=1}^{n} \langle c_i, \vec{u}} \rangle^3 c_i\]
where $\mu_4$ is the 4th moment of $z_i \in \vec{z}$, i.e. $\bb{E}[z^4]$, and $\vec{u}$ is a vector on the unit sphere of $\bb{R}^{2n}$, i.e. $\lVert \vec{u} \rVert = 1$. 
% \footnote{Note that in the previous chapter about HPP, $\vec{w}$ was used to denote a vector on the unit sphere of $\bb{R}^{2n}$.
% Here we denote by $\vec{u}$ such a vector as to not confuse it with our current meaning of $\vec{w}$, which denotes a signature $\vec{w} = \mat{B}^{-1} \vec{x}$}
This means that if the difference $(\mu_4 - 3)$ determines the applicability of the attack. 
As we showed in section 3.6, the attack does not work for a normal distribution, i.e. a distribution where $\mu_4 = 3$.
However, if $\mu_4 \neq 3$, and the difference $(\mu_3 - 3)$ is significant enough, one might be able to employ the same minimization technique 
as in the original attack to reveal a column of $\pm \mat{C}$.

Independent of what the underlying distribution is, $\langle c_i, \vec{u} \rangle^4 = 1$ if $\vec{u} = \pm c_i$ since $\langle c_i, c_i \rangle^4 = 1$, so the crux of the attack still lies in
optimizing the $mom_{4, \mat{C}}(\vec{u})$ function w.r.t. the vector $\vec{w}$ to reveal columns of $\pm \mat{C}$.
Note that if $(\mu_4 - 3) < 0$ we have the same case as in the original attack, where minimization of the entire term entails maximization of $\mathlarger{\sum_{i=1}^{n} \langle c_i, \vec{u} \rangle^4}$, which gives us a row of $\pm \mat{C}$.
If $(\mu_4 - 3) > 0$, we need to maximize the entire term $3 \lVert \vec{u} \rVert ^4 + \mathlarger{\sum_{i=1}^{n} \langle c_i, \vec{u} \rangle^4}$, which is achieved by doing a gradient \textit{ascent} instead of a gradient \textit{descent}.

We approximate the functions $mom_{4, \mat{C}}(\vec{u})$ as $\bb{E} [\langle \vec{c}, \vec{u} \rangle ^4]$ and $\nabla mom_{4,\mat{C}}(\vec{u})$ as $\bb{E} [\nabla \langle \vec{c}, \vec{u} \rangle ^4] = 4 \bb{E} [\langle \vec{c}, \vec{u} \rangle ^3 \vec{c}]$.

% These can be evaluated by precomputed samples $\{\vec{c}_1, \vec{c}_2, ..., \vec{c}_t\}$, or by continuously generating one and one signature sample since we have access to the signature generation algorithm.
% Below, gradient descent adapted to is described as an algorithm, similar to . Note that the only difference is in what direction to move and the condition for termination, on lines 4 and 6 respectively.
In Algorithm \ref{hawk_gradient_search} the basic gradient descent is described for the Hawk setting. Note that 
for a gradient \textit{ascent} one only needs to flip the sign and inequality sign on lines 4 and 6, respectively.
\begin{algorithm}[H]
    \caption{Gradient descent on $\PP{C}$}\label{hawk_gradient_search}
\begin{algorithmic}[1]
    \Require{Samples on the form $\vec{c} = \mat{C}\vec{z}$, descent parameter $\delta$}
    \State $\vec{u} \gets \text{ random vector on unit sphere in } \bb{R}^{2n}$
    \Loop
    \State $\vec{g} \gets \nabla \mom{4}{C}$
    \State $\vec{u}_{new} = \vec{u} - \delta \vec{g}$
    \State normalize $\vec{u}_{new}$ as $\frac{\vec{u}_{new}}{\lVert \vec{u}_{new} \rVert}$
    \If{$mom_{4, \mat{C}}(\vec{u}_{new}) \geq mom_{4, \mat{C}(\vec{u})}$}
    \State \Return{$\vec{u}$}
    \Else 
    \State $\vec{u} \gets \vec{u}_{new}$
    \State go to step 3
    \EndIf
    \EndLoop
\end{algorithmic}
\end{algorithm}

\section{Practical method}

Before analyzing the discrete Gaussian Distribution to determine the value $\mu_4$ which will provide a theoretical answer for whether the method will work or not, we present the attack against Hawk in a practical setting.
We also present a simple method for measuring this method experimentally.

Below is a basic description of the approach.
\begin{algorithm}[H]
\caption{Proposed high-level version of attack}
\begin{algorithmic}[1]
    \State Collect signatures $\vec{w} = \mat{B}^{-1} \vec{x}$
    \State Using public key $\mat{Q}$, find $\mat{L}$ s.t. $\mat{Q} = \mat{L} \mat{L}^T$
    \State Transform samples s.t. $\vec{c} = \mat{L}^T \mathlarger{\frac{\vec{w}}{\sigma}}$
    \State Find columns of $\pm \mat{C}$ by doing gradient search over $\PP{C}$
    \State Multiply columns of $\pm \mat{C}$ by $\mat{L}^{-T}$ on the left and round the result to get columns in $\pm \mat{B}^{-1}$
\end{algorithmic}
\end{algorithm}

Step 1 of the algorithm is concerned with collecting a number of signatures. We will use the variable $t$ to denote number of signature samples used.
Recall that a proper signature is on the form $\vec{s} = \frac{1}{2}(\vec{h} - \vec{w})$ where $\vec{w} = \mat{B}^{-1} \vec{x}$. Since anyone is able to recompute $\vec{w}$ by observing a message signature pair 
($\vec{m}, \vec{s}$ and a salt $\mathsf{salt}$), for our experiments we run a version of the signature generation algorithm that directly returns the vector $\vec{w}$.
Also, each signature needs some input message. In the implementation an array of 100 random bytes ($256^{100}$ different combinations) is used as a message, which is sufficient to have unique message inputs.
For each message we store the signature $\vec{w}$ and discard the message $\vec{m}$, since we only need $\vec{w}$.

Each signature sample is originally stored as a vector of signed 16-bit integers. Then to store $t$ signature samples one would need $t \cdot 2n \cdot 16$ bits $= t \cdot 2n \cdot 2$ bytes.
However, in the next step of the algorithm we need to transform the samples on to the hidden hypercube, using the matrix $\mat{L}^{T}$. This matrix is not an integer matrix, and thus we need to use floating point numbers 
to represent it. Consequently, the signature samples would also need to be represented by floating point numbers.
Using the Rust programming language and a linear algebra crate Nalgebra \cite{Nalgebra} it quickly became evident that the standard 32-bit floating point numbers were not well enough supported in this library.
So, using 64-bit floating point numbers, each signature sample now takes $t \cdot 2n \cdot 8$ bytes. For example, for $t = 10^6$ and challenge parameter $n=256$ one needs about $4.096 \cdot 10^{9}$ bytes, i.e. 4GB of memory purely
for the storage of the samples.

An alternative to storing signature samples from the start of the attack, is to simply generate new independent signature samples on demand when approximating $mom_{4, \mat{C}}(\vec{u})$ and its gradient. 
This removes the requirement for memory, but this tradeoff imposes a much slower running time on the entire attack. Luckily, for the purposes of this thesis, I obtained access to a cloud computer through NREC (Nordic Research and Education Cloud)
with 512 GB RAM. This made experiments much less cumbersome when memory was practically not a concern, as opposed to running experiments on a personal computer with 16 GB RAM. Nevertheless, the fact that the attack \textit{can} theoretically
be memory-efficient is a big advantage of the attack.

% After having transformed the samples $\vec{w} \in \PP{\mat{B}^{-1}}$ to $\vec{c} \in \PP{\mat{C}}$ we want to recover columns of $\pm \mat{C}$ and transform them back to columns of $\pm \mat{B}^{-1}$ by multiplying by $\mat{L}^{-T}$ on the left.
Due to the special structure of Hawk private keys, finding one column of $\mat{B}$ automatically gives $n$ columns.
Since revealing a single column of $ \mat{B}^{-1}$ reveals a "shift" of either the two polynomials $G$ and $ g$ \textit{or} $F$ and $f$,
this is not enough to disclose the entire matrix. If samples were on the form $\vec{w} = \mat{B} \vec{x}$, a single column would reveal $f$ and $g$ (or $F$ and $G$), and one could simply reconstruct $F$ and $G$ (or $f$ and $g$)
by solving the NTRU-equation as in the key generation step of Hawk.
Nevertheless, if one finds two columns of $\mat{B}^{-1}$, it is easy to check if they are shifts of each other. If they are not, one has found shifts of all four polynomials in the secret key, 
and by trying all combinations of shifts, of which there are $4 n^2$ (accounting for negative and positive sign), one can easily verify if a candidate $\mat{B'}$ is valid by 
checking if $\mat{B}'$ is unimodular and if $\mat{B'}^T \mat{B'} = \mat{Q}$. If so, one is able to forge signatures, and the attack is done.

After each gradient ascent and/or descent returns a possible solution $\vec{z}'$, 
we multiply it to the left as $\vec{b}' = \mat{L}^{-T} \vec{z}'$ where $\vec{b}'$ is a possible column of $\pm \mat{B}^{-1}$ as $\vec{z}'$ is a possible column of $\pm \mat{C} = \mat{L}^T \cdot (\pm \mat{B}^{-1})$.
Since in experiments we have access to the correct secret key, we simply check directly if $\mat{b}' \in \pm \mat{B}^{-1}$.
In a real word attack however, one would, as described above, have to compute candidate $\mat{B}'$ and check if $\mat{B}'^T\mat{B}' = \mat{Q}$.
\todo{What to do with this paragraph}

\subsection{Measuring method}
Having access to the correct $\mat{B}^{-1}$ we can do measures on how close a proposed solution $\vec{b}'$ is to one of the columns of $\mat{B}^{-1}$.
We can for example take the difference in length of $\lvert \vec{b}' \rvert $ and each vector in $\lvert \mat{B}^{-1}\rvert$, i.e. 
$ \mathsf{diff_{min}} = \mathsf{min} \{\lVert \lvert \vec{b}' \rvert - \lvert \vec{b}_i \rvert \rVert \ \mathbf{:} \ \vec{b}_i \in \pm \mat{B}^{-1}\}$.
A low value for this (this depends on the Hawk parameter $n$) would indicate a good guess, whereas a higher value indicates greater difference between the vectors, and thus a bad guess.
% One can also take the average of the difference of the entries in the vectors, i.e. $ \mathsf{diff_{min}} = \mathsf{min} \{\frac{1}{2n} | \vec{b}' - \vec{b}_i | \mathbf{:} \vec{b}_i \in \pm \mat{B}^{-1}\}$.
Alternatively, one can count entrywise how many entries out of $2n$ that matches between $\vec{b}'$ and $\vec{b}_i$, or use some other suitable measurement.

Below is a more detailed version of the entire attack/experiment.
\begin{algorithm}[H]
\caption{Proposed version of attack with measuring}
\begin{algorithmic}[1]
    \Require{Hawk parameter $n$, number of samples $t$, Hawk key pair $\mat{B}$, $\mat{Q}$}
    \State Collect $t$ signatures $\vec{w} = \mat{B}^{-1} \vec{x}$ \Comment{Optional - can also generate signatures continuously}
    \State Using public key $\mat{Q}$, compute $\mat{L}$ s.t. $\mat{Q} = \mat{L} \mat{L}^t$
    \State Transform samples s.t. $\vec{c} = \mat{L}^t \vec{w}$
    \Loop
    \State Candidate $\vec{z}' \gets \mathsf{gradient \ search}$ \Comment{Do both ascent and descent}
    \State Candidate $\vec{b}' \gets \lfloor \mat{L}^{-T} \vec{z}' \rceil$ \Comment{Entrywise rounding to nearest integer}
    \State Check if $\vec{b}' \in \mat{B}^{-1}$
    \State Measure $\mathsf{diff_{min}}(\vec{b}', \mat{B}^{-1})$
    \EndLoop
\end{algorithmic}
\end{algorithm}

The loop from line 4 can be run several times to get a new random starting point $\vec{u}$ for the gradient search.
Line 6 rounds the candidate \textit{after} multiplying with $\mat{L}^{-T}$ to avoid rounding errors.
Line 8 can also give which column of $\mat{B}^{-1}$ $\vec{b}'$ is closest to, so one can compare the vectors side by side.


\section{Estimating $\mu_4$}
\subsection{Estimating $\mu_4$ via direct sampling}
Consider the Discrete Gaussian Distribution as described in \cite{HawkSpec24} and section 3.1.5. We can use our implementation of Hawk to sample many points from the distribution.
Let $\dgdi$ denote the practical discrete Gaussian distribution from sampled points.
Let $\hat{\mu}$, $\hat{\sigma}^2$ be the expectation and variance of $\dgdi$.
Assume we sample $t$ points from $\dgdi$ as $X = \{x_1, x_2, ..., x_t\}$. We estimate $\hat{\mu}$ and $\hat{\sigma}^2$ simply as $\hat{\mu} = \mathlarger{\frac{1}{t} \sum_{i=1}^{t} x_i}$ and $\hat{\sigma}^2 = \mathlarger{\frac{1}{t} \sum_{i=1}^{t}(x_i - \hat{\mu})^2}$.
For simplicity, we can also assume $\hat{\mu} = 0$ as claimed in \cite{HawkSpec24}.
To simplify later computations we also normalize our samples by computing $Z = \{z_1, z_2, ..., z_t\} = \{\frac{x_1}{\hat{\sigma}}, \frac{x_2}{\hat{\sigma}},..., \frac{x_t}{\hat{\sigma}}\}$ such that 
$\bb{V}[z_i] = 1$.
Now $\hat{\mu}_4 = \bb{E}[z_i^4] = \mathlarger{\frac{1}{t} \sum_{i=1}^{n} z_i ^4}$. 

The problem with this approach is the requirement of many samples in order to determine $\hat{\mu}_4$ with high enough accuracy.
By using Confidence Intervals and Central Limit Theorem, we get a number of how many samples one should sample in order for $\hat{\mu}_4$ to be accurate.

\[
    n = (\frac{z_{\alpha / 2} \cdot \hat{\sigma}}{E} )^2    
\]
where $E$ is the acceptable error and $z_{\alpha / 2}$ is found in standard normal tables.
Assume one wants to determine with confidence $95 \%$ that an estimate $\hat{\mu}_4$ differs from the real $\mu_4$ by maximum $0.00001$, or $1 \times 10^{-5}$.
Then $n = (\frac{1.96 \cdot \hat{\sigma}} {10^{-5}})^2$
\todo{Show this value maybe??}

\subsection{Computing $\mu_4$ analytically}
The probability that one can discern $mom_{4, \mat{C}}(\vec{u})$ when $\vec{u} \in \pm \mat{C}$ and when $\vec{u} \not\in \pm \mat{C}$ ultimately depends on the value of $\mu_4$ and the number of signature samples one has available.
% If we can upper estimate the variance of $mom_{4, \mat{C}}(\vec{u})$ and $\nabla mom_{4, \mat{C}}(\vec{u})$ one can get an idea of how many samples one needs to do this distinguishment.

For implementation of Hawk, they provide tables which along with an algorithm determines the distribution $\dgdi_{2\bb{Z} + c, \sigma}$. Based on the tables one can reconstruct a 
Probability Mass Function (PMF) $Pr(X = x)$ for $X \sim \dgdi_{\bb{Z}, \sigma}$.
The PMF of $\dgdi_{\bb{Z}, \sigma}$ from tables is:
\[
    Pr(X=x) = \frac{1}{2} \cdot
\begin{cases}
    1 - T_c[0] & \text{, if } \lvert x \rvert = c \\
    \frac{1}{2}\mathlarger{(T_c[\frac{\lvert x \rvert - c}{2} - 1] - T_c[\frac{\lvert x \rvert - c}{2}])} & \text{, if } \lvert x \rvert > c
\end{cases}
\]
Here, $c = \lvert x \rvert \mod{2}$ that determines which table ($T_0$ or $T_1$) is used. \todo{Explain this stuff a bit better maybe}

Given the PMF we can compute the moments of $X$ as $\bb{E}[X^k] = \mathlarger{\sum_x x^k \cdot Pr(X = x)}$, as there is a finite number of entries in the tables.
More specifically, for the 256 parameter set, there are 10 entries in $T_0$ and $T_1$. Therefore we sum from $x = -20$ to $x = 20$.
When $|x| = 20$, $Pr(\lvert X \rvert \geq 20) = \frac{1}{4} \cdot (T_0 [9] - T_0[10])$, where $T_0[9]$ is the last non-zero entry, and so all successive entries will yield 0.
Similarly, for parameters 512 and 1024 we sum from $x = -26$ to $x = 26$, since there are 13 entries in $T_0$ and $T_1$.

By using the Decimal module in Python \cite{Decimal}, a module designed for high precision when working with decimal numbers, we can be confident that the computed results are correct, 
and that we do not get e.g. floating point errors when summing, since we are dealing with very small numbers.

We compute $\mu_4$ as $\mu_4 = \bb{E}[Z^4] = \bb{E}[(\frac{X}{\sigma})^4] = \frac{1}{\sigma^4}\bb{E}[X^4]$.
In table \ref{z4 values} we show the value of $\mu_4$ and $\mu_4 - 3$ rounded at 25 and 39 decimal points, respectively, for all three Hawk degrees.

\begin{table}[H]
    \centering
    \caption{Measure of $\mu_4$}
    \label{z4 values}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Degree} & $\bb{E}[Z^4]$ & $\bb{E}[Z^4] - 3$ \\
        \midrule
        256 & 2.9999999999999790015752619 & -2.0998424738064930232320331 $\cdot 10^{-14}$ \\
        \midrule
        512 & 2.9999999999999999999862684 & -1.3731593731486976093589903 $\cdot 10^{-20}$ \\
        \midrule
        1024 & 2.999999999999999999987558 & -1.2441396421557256288414318 $\cdot 10^{-20}$ \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Alternative method}
An observation about the Hawk signatures is that one can collect some $\vec{w}_1 = \mat{B}^{-1} \vec{x}_1$ where $\vec{x}_1$ may have a different distribution than $\vec{x}$.
Recall that in the signature generation algorithm one computes $\vec{t} = \mat{B} \vec{h}$ mod 2. This is equivalent to taking $\vec{t} = \mat{B} \vec{h} + 2 \vec{d}$ where $\vec{d}$ is some integer vector.
Moreover, $\vec{x} = \vec{t} + 2 \vec{y}$ where $\vec{y}$ is some integer vector as well.
Now, $\vec{w} = \mat{B}^{-1} \vec{x} = \mat{B}^{-1}(\mat{B} \vec{h} + 2 \vec{d} + 2 \vec{y})$. Simplifying, we get that $\mathlarger{\frac{\vec{w} - \vec{h}}{2}} = \mat{B}^{-1}(\vec{d} + \vec{y})$.

We can easily deduce the distribution that $\vec{y}$ follows.
If we can somehow determine what distribution the vector $(\vec{d} + \vec{y})$, and show that it is not too close to normal, one may recover columns of $\mat{B}^{-1}$ this way.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{hpp_parallelepiped_normal_discrete.png}
    \caption{Hidden parallelepiped in dimension 2 for discrete Gaussian distribution ($\sigma = 2.02$)}
  	\medskip 
	% \hspace*{15pt}\hbox{\scriptsize Credit: Acme company makes everything \url{https://acme.com/}}
    \label{parallelepiped_normal_discrete}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{hppnormal_cube_discrete.png}
    \caption{Hidden hypercube in dimension 2 for discrete Gaussian distribution ($\sigma = 2.02$)}
  	\medskip 
	% \hspace*{15pt}\hbox{\scriptsize Credit: Acme company makes everything \url{https://acme.com/}}
    \label{hypercube_normal_discrete}
\end{figure}
}

\newtheorem{hpp_problem_hawk}{Problem}

\chapter{Cryptanalysis of Hawk}
\section{Overview}
In this chapter we introduce our cryptanalysis of Hawk by adapting the original HPP attack to the Hawk setting.
As previously shown, a continuous normal distribution is immune against such an attack due to the constant 4-th moment. In Hawk, however, the distribution of entries in $\vec{x}$ is discrete, not continuous.
We will show in section 4.2 that the Discrete Gaussian Distribution based on tables and the algorithm \ref{sample} is indeed not identical to a continuous normal distribution, and that an attack akin to the original HPP may still be applicable.

In section 4.3, we explain the procedure to transform the hidden parallelepiped to a hidden hypercube, and show why this is a trivial task for Hawk signatures.
Section 4.4 will cover the main part of the attack, namely the gradient descent, which will in many ways be similar to the original attack.
Lastly, section 4.5 will present the complete practical method and give details about its implementation.

\section{Redefining the problem and solution}

We now restate the Hidden Parallelepiped Problem in the Hawk setting.
Recall in HPP that one observes row vectors $\vec{v} = \vec{x} \mat{V}$. In the Hawk setting, the observed signature is $\vec{w} = \mat{B}^{-1} \vec{x}$ since we have moved to column-notation for vectors. 
The problem to solve is formulated in Problem \ref{HPP problem Hawk}

\begin{hpp_problem_hawk}\label{HPP problem Hawk}
Given signature samples on the form $\vec{w} = \mat{B}^{-1} \vec{x}$ where only $\vec{w}$ is known, $\mat{B}$ is a Hawk private key and $\vec{x}$ is distributed according to the discrete Gaussian Distribution $\dgd_{2\bb{Z}^{2n} + \vec{t}, \sigma}$,
recover columns of $\mat{B}$.
\end{hpp_problem_hawk}
The general steps in Hawk setting will be similar to that of the original HPP:
\begin{itemize}
    \item Compute Covariance Matrix $(\mat{B}^{-1})^T (\mat{B}^{-1})$
    \item Transform the samples $\vec{w} \in \PP{\mat{B}^{-1}}$ to $\vec{c} \in \PP{C}$ where $\mat{C}$ is orthonormal
    \item Deduce columns of $\mat{C}$ doing gradient descent of minimizing the 4th moment of one-dimensional projections of $\PP{C}$
\end{itemize}

Note that in this section we denote by $\vec{u}$ a vector on the unit sphere $\bb{R}^{2n}$ instead of $\vec{w}$ as in the previous section, to avoid confusion with the Hawk notation for $\vec{w} = \mat{B}^{-1} \vec{x}$.
Also note that henceforth we will consider only $\vec{w} = \mat{B}^{-1} \vec{x}$ as a signature instead of $\vec{s} = \frac{1}{2}(\vec{h} - \vec{w})$ since $\vec{w}$ is easily recoverable given $\vec{s}$ and message $\vec{m}$.
Lastly, we consider $\mat{B}$ as $\mathsf{rot}(\mat{B})$ and thus work with matrices in $\bb{Q}^{2n \times 2n}$ and column vectors in $\bb{Q}^{2n}$.

\subsection{Covariance matrix and hypercube transformation}
In the original HPP attack one has to estimate the matrix $\mat{G} \approx \mat{V} ^t \mat{V}$.
For Hawk, the signatures are on the form $\vec{w} = \mat{B}^{-1} \vec{x}$ which means we would need to compute $\mat{G} = \mat{B}^{-1} \mat{B}^{-T}$ 
(recall the HPP paper uses row vectors while Hawk use columns vectors).
However, the public key $\mat{Q} = \mat{B}^T \mat{B}$, enables us to skip this step.
In the original attack one takes Cholesky decomposition of the inverse of the covariance matrix such that $\mat{G}^{-1} = \mat{L}\mat{L}^T$. 
For $\mat{G} = \mat{B}^{-1} \mat{B}^{-T}$, the inverse,
$\mat{G}^{-1} = \mat{B}^T \mat{B} = \mat{Q}$. Therefore, we can simply take the Cholesky decomposition of $\mat{Q}$ to get $\mat{L} \text{ such that } \mat{Q} = \mat{L} \mat{L}^T$.
By multiplying our samples $\vec{w}$ by $\mat{L}^T$ on the left, we have transformed our samples to the hidden hypercube as in the original attack. \\
By taking $\mat{C} =  \mat{L}^T\mat{B}^{-1}$, we have that 
\[\mat{C}^T \mat{C} = (\mat{L}^T \mat{B}^{-1})^T (\mat{L}^T \mat{B}^{-1}) = \mat{B}^{-T} \mat{L} \mat{L}^T \mat{B}^{-1} = \mat{B}^{-T} \mat{Q} \mat{B}^{-1} = \mat{B}^{-T} \mat{B}^{T} \mat{B} \mat{B}^{-1} = \mat{I}_n\]
and
\[\mat{C}\mat{C}^T = (\mat{L}^T \mat{B}^{-1})(\mat{L}^T \mat{B}^{-1})^T = \mat{L}^T \mat{B}^{-1} \mat{B}^{-T} \mat{L} =  \mat{L}^T \mat{Q}^{-1} \mat{L} = \mat{L}^T (\mat{L} \mat{L}^T)^{-1} \mat{L} = \mat{L}^T \mat{L}^{-T} \mat{L}^{-1} \mat{L} = \mat{I}_n\]
Thus $\mat{C}$ is an orthonormal matrix.
Since $\vec{w}$ is distributed according to $\dgdi$ over $\PP{\mat{B}^{-1}}$, by taking 
$\vec{c} = \mat{L}^{T} \vec{w}$ we have $\vec{c} = \mat{L}^{T} \mat{B}^{-1} \vec{x} = \mat{C} \vec{x}$, $\vec{c}$ is distributed according to $\dgdi$ over $\PP{C}$.
Lastly, we also want to normalize the distribution of entries in $\vec{x}$ to have variance 1. Therefore we consider instead
$\vec{c} = \mat{C} \vec{z} = \mathlarger{\frac{\mat{C}\vec{x}}{\sigma}}$ where $\sigma$ is the std.dev. of $\dgd_{2\bb{Z} + \vec{t}, \sigma}$.
% , by dividing our samples $\vec{w}$ by $\sigma$ in addition to multiplying them with $\mat{L}^{-1}$.
We summarize this procedure in the following algorithm:

\begin{algorithm}
\caption{Hawk Hypercube Transformation}
\begin{algorithmic}[1]
% \Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
    \Require{Samples $\vec{w} = \mat{B}^{-1} \vec{x}$ and public key $\mat{Q}$}
    \State Compute $\mat{L}$ s.t. $\mat{Q} = \mat{L}\mat{L}^T$ \Comment by Cholesky decomposition
    \State Compute $\vec{c} = \mat{L}^T \mathlarger{\frac{\vec{w}}{\sigma}}$ \Comment entrywise division by $\sigma$
    \State \Return{$\vec{c}$ and $\mat{L}^{-T}$}
\end{algorithmic}
\end{algorithm}

\subsection{Moments and gradient search}
Assume now observed signatures on the form $\vec{c} = \mat{C} \vec{z}$ where $\mat{C}$ is orthonormal, and the normalized distribution entries in $\vec{z}$ follows have variance 1. 
By rewriting the terms from section 3.3.4 for this distribution, we have that
\[mom_{4, \mat{C}} (\vec{u}) = 3 \lVert \vec{u} \rVert ^4 + (\mu_4 - 3) \sum_{i=1}^{n} \langle c_i, \vec{u} \rangle^4 \]
and
\[\nabla mom_{4, \mat{C}} (\vec{u}) = 12 \lVert \vec{u} \rVert^2 \vec{u} + 4(\mu_4 - 3) \mathlarger{\sum_{i=1}^{n} \langle c_i, \vec{u}} \rangle^3 c_i\]
where $\mu_4$ is the 4th moment of $z_i \in \vec{z}$, i.e. $\bb{E}[z^4]$, and $\vec{u}$ is a vector on the unit sphere of $\bb{R}^{2n}$, i.e. $\lVert \vec{u} \rVert = 1$. 
% \footnote{Note that in the previous chapter about HPP, $\vec{w}$ was used to denote a vector on the unit sphere of $\bb{R}^{2n}$.
% Here we denote by $\vec{u}$ such a vector as to not confuse it with our current meaning of $\vec{w}$, which denotes a signature $\vec{w} = \mat{B}^{-1} \vec{x}$}
This means that if the difference $(\mu_4 - 3)$ determines the applicability of the attack. 
As we showed in section 3.6, the attack does not work for a normal distribution, i.e. a distribution where $\mu_4 = 3$.
However, if $\mu_4 \neq 3$, and the difference $(\mu_3 - 3)$ is significant enough, one might be able to employ the same minimization technique 
as in the original attack to reveal a column of $\pm \mat{C}$.

Independent of what the underlying distribution is, $\langle c_i, \vec{u} \rangle^4 = 1$ if $\vec{u} = \pm c_i$ since $\langle c_i, c_i \rangle^4 = 1$, so the crux of the attack still lies in
optimizing the $mom_{4, \mat{C}}(\vec{u})$ function w.r.t. the vector $\vec{w}$ to reveal columns of $\pm \mat{C}$.
Note that if $(\mu_4 - 3) < 0$ we have the same case as in the original attack, where minimization of the entire term entails maximization of $\mathlarger{\sum_{i=1}^{n} \langle c_i, \vec{u} \rangle^4}$, which gives us a row of $\pm \mat{C}$.
If $(\mu_4 - 3) > 0$, we need to maximize the entire term $3 \lVert \vec{u} \rVert ^4 + \mathlarger{\sum_{i=1}^{n} \langle c_i, \vec{u} \rangle^4}$, which is achieved by doing a gradient \textit{ascent} instead of a gradient \textit{descent}.

We approximate the functions $mom_{4, \mat{C}}(\vec{u})$ as $\bb{E} [\langle \vec{c}, \vec{u} \rangle ^4]$ and $\nabla mom_{4,\mat{C}}(\vec{u})$ as $\bb{E} [\nabla \langle \vec{c}, \vec{u} \rangle ^4] = 4 \bb{E} [\langle \vec{c}, \vec{u} \rangle ^3 \vec{c}]$.

% These can be evaluated by precomputed samples $\{\vec{c}_1, \vec{c}_2, ..., \vec{c}_t\}$, or by continuously generating one and one signature sample since we have access to the signature generation algorithm.
% Below, gradient descent adapted to is described as an algorithm, similar to . Note that the only difference is in what direction to move and the condition for termination, on lines 4 and 6 respectively.
In Algorithm \ref{hawk_gradient_search} the basic gradient descent is described for the Hawk setting. Note that 
for a gradient \textit{ascent} one only needs to flip the sign and inequality sign on lines 4 and 6, respectively.
\begin{algorithm}[H]
    \caption{Gradient descent on $\PP{C}$}\label{hawk_gradient_search}
\begin{algorithmic}[1]
    \Require{Samples on the form $\vec{c} = \mat{C}\vec{z}$, descent parameter $\delta$}
    \State $\vec{u} \gets \text{ random vector on unit sphere in } \bb{R}^{2n}$
    \Loop
    \State $\vec{g} \gets \nabla \mom{4}{C}$
    \State $\vec{u}_{new} = \vec{u} - \delta \vec{g}$
    \State normalize $\vec{u}_{new}$ as $\frac{\vec{u}_{new}}{\lVert \vec{u}_{new} \rVert}$
    \If{$mom_{4, \mat{C}}(\vec{u}_{new}) \geq mom_{4, \mat{C}(\vec{u})}$}
    \State \Return{$\vec{u}$}
    \Else 
    \State $\vec{u} \gets \vec{u}_{new}$
    \State go to step 3
    \EndIf
    \EndLoop
\end{algorithmic}
\end{algorithm}

\section{Practical method}

Below is a basic description of the approach.
\begin{algorithm}[H]
\caption{Proposed high-level version of attack}
\begin{algorithmic}[1]
    \State Collect signatures $\vec{w} = \mat{B}^{-1} \vec{x}$
    \State Using public key $\mat{Q}$, find $\mat{L}$ s.t. $\mat{Q} = \mat{L} \mat{L}^T$
    \State Transform samples s.t. $\vec{c} = \mat{L}^T \mathlarger{\frac{\vec{w}}{\sigma}}$
    \State Find columns of $\pm \mat{C}$ by doing gradient search over $\PP{C}$
    \State Multiply columns of $\pm \mat{C}$ by $\mat{L}^{-T}$ on the left and round the result to get columns in $\pm \mat{B}^{-1}$
\end{algorithmic}
\end{algorithm}

% After having transformed the samples $\vec{w} \in \PP{\mat{B}^{-1}}$ to $\vec{c} \in \PP{\mat{C}}$ we want to recover columns of $\pm \mat{C}$ and transform them back to columns of $\pm \mat{B}^{-1}$ by multiplying by $\mat{L}^{-T}$ on the left.
Due to the special structure of Hawk private keys, finding one column of $\mat{B}$ automatically gives $n$ columns.
Since revealing a single column of $ \mat{B}^{-1}$ reveals a "shift" of either the two polynomials $G$ and $ g$ \textit{or} $F$ and $f$,
this is not enough to disclose the entire matrix. If samples were on the form $\vec{w} = \mat{B} \vec{x}$, a single column would reveal $f$ and $g$ (or $F$ and $G$), and one could simply reconstruct $F$ and $G$ (or $f$ and $g$)
by solving the NTRU-equation as in the key generation step of Hawk.
Nevertheless, if one finds two columns of $\mat{B}^{-1}$, it is easy to check if they are shifts of each other. If they are not, one has found shifts of all four polynomials in the secret key, 
and by trying all combinations of shifts, of which there are $4 n^2$ (accounting for negative and positive sign), one can easily verify if a candidate $\mat{B'}$ is valid by 
checking if $\mat{B}'$ is unimodular and if $\mat{B'}^T \mat{B'} = \mat{Q}$. If so, one is able to forge signatures, and the attack is done.

After each gradient ascent and/or descent returns a possible solution $\vec{z}'$, 
we multiply it to the left as $\vec{b}' = \mat{L}^{-T} \vec{z}'$ where $\vec{b}'$ is a possible column of $\pm \mat{B}^{-1}$ as $\vec{z}'$ is a possible column of $\pm \mat{C} = \mat{L}^T \cdot (\pm \mat{B}^{-1})$.
Since in experiments we have access to the correct secret key, we simply check directly if $\mat{b}' \in \pm \mat{B}^{-1}$.
In a real word attack however, one would, as described above, have to compute candidate $\mat{B}'$ and check if $\mat{B}'^T\mat{B}' = \mat{Q}$.
\todo{What to do with this paragraph}

\subsection{Measuring method}
Having access to the correct $\mat{B}^{-1}$ we can do measures on how close a proposed solution $\vec{b}'$ is to one of the columns of $\mat{B}^{-1}$.
We can for example take the difference in length of $\lvert \vec{b}' \rvert $ and each vector in $\lvert \mat{B}^{-1}\rvert$, i.e. 
$ \mathsf{diff_{min}} = \mathsf{min} \{\lVert \lvert \vec{b}' \rvert - \lvert \vec{b}_i \rvert \rVert \ \mathbf{:} \ \vec{b}_i \in \pm \mat{B}^{-1}\}$.
A low value for this (this depends on the Hawk parameter $n$) would indicate a good guess, whereas a higher value indicates greater difference between the vectors, and thus a bad guess.
% One can also take the average of the difference of the entries in the vectors, i.e. $ \mathsf{diff_{min}} = \mathsf{min} \{\frac{1}{2n} | \vec{b}' - \vec{b}_i | \mathbf{:} \vec{b}_i \in \pm \mat{B}^{-1}\}$.
Alternatively, one can count entrywise how many entries out of $2n$ that matches between $\vec{b}'$ and $\vec{b}_i$, or use some other suitable measurement.

Below is a more detailed version of the entire attack/experiment.
\begin{algorithm}[H]
\caption{Proposed version of attack with measuring}
\begin{algorithmic}[1]
    \Require{Hawk parameter $n$, number of samples $t$, Hawk key pair $\mat{B}$, $\mat{Q}$}
    \State Collect $t$ signatures $\vec{w} = \mat{B}^{-1} \vec{x}$ \Comment{Optional - can also generate signatures continuously}
    \State Using public key $\mat{Q}$, compute $\mat{L}$ s.t. $\mat{Q} = \mat{L} \mat{L}^t$
    \State Transform samples s.t. $\vec{c} = \mat{L}^t \vec{w}$
    \Loop
    \State Candidate $\vec{z}' \gets \mathsf{gradient \ search}$ \Comment{Do both ascent and descent}
    \State Candidate $\vec{b}' \gets \lfloor \mat{L}^{-T} \vec{z}' \rceil$ \Comment{Entrywise rounding to nearest integer}
    \State Check if $\vec{b}' \in \mat{B}^{-1}$
    \State Measure $\mathsf{diff_{min}}(\vec{b}', \mat{B}^{-1})$
    \EndLoop
\end{algorithmic}
\end{algorithm}

The loop from line 4 can be run several times to get a new random starting point $\vec{u}$ for the gradient search.
Line 6 rounds the candidate \textit{after} multiplying with $\mat{L}^{-T}$ to avoid rounding errors.
Line 8 can also give which column of $\mat{B}^{-1}$ $\vec{b}'$ is closest to, so one can compare the vectors side by side.


\section{Estimating $\mu_4$}
\subsection{Estimating $\mu_4$ via direct sampling}
Consider the Discrete Gaussian Distribution as described in \cite{HawkSpec24} and section 3.1.5. We can use our implementation of Hawk to sample many points from the distribution.
Let $\dgdi$ denote the practical discrete Gaussian distribution from sampled points.
Let $\hat{\mu}$, $\hat{\sigma}^2$ be the expectation and variance of $\dgdi$.
Assume we sample $t$ points from $\dgdi$ as $X = \{x_1, x_2, ..., x_t\}$. We estimate $\hat{\mu}$ and $\hat{\sigma}^2$ simply as $\hat{\mu} = \mathlarger{\frac{1}{t} \sum_{i=1}^{t} x_i}$ and $\hat{\sigma}^2 = \mathlarger{\frac{1}{t} \sum_{i=1}^{t}(x_i - \hat{\mu})^2}$.
For simplicity, we can also assume $\hat{\mu} = 0$ as claimed in \cite{HawkSpec24}.
To simplify later computations we also normalize our samples by computing $Z = \{z_1, z_2, ..., z_t\} = \{\frac{x_1}{\hat{\sigma}}, \frac{x_2}{\hat{\sigma}},..., \frac{x_t}{\hat{\sigma}}\}$ such that 
$\bb{V}[z_i] = 1$.
Now $\hat{\mu}_4 = \bb{E}[z_i^4] = \mathlarger{\frac{1}{t} \sum_{i=1}^{n} z_i ^4}$. 

The problem with this approach is the requirement of many samples in order to determine $\hat{\mu}_4$ with high enough accuracy.
By using Confidence Intervals and Central Limit Theorem, we get a number of how many samples one should sample in order for $\hat{\mu}_4$ to be accurate.

\[
    n = (\frac{z_{\alpha / 2} \cdot \hat{\sigma}}{E} )^2    
\]
where $E$ is the acceptable error and $z_{\alpha / 2}$ is found in standard normal tables.
Assume one wants to determine with confidence $95 \%$ that an estimate $\hat{\mu}_4$ differs from the real $\mu_4$ by maximum $0.00001$, or $1 \times 10^{-5}$.
Then $n = (\frac{1.96 \cdot \hat{\sigma}} {10^{-5}})^2$
\todo{Show this value maybe??}

\subsection{Computing $\mu_4$ analytically}
The probability that one can discern $mom_{4, \mat{C}}(\vec{u})$ when $\vec{u} \in \pm \mat{C}$ and when $\vec{u} \not\in \pm \mat{C}$ ultimately depends on the value of $\mu_4$ and the number of signature samples one has available.
% If we can upper estimate the variance of $mom_{4, \mat{C}}(\vec{u})$ and $\nabla mom_{4, \mat{C}}(\vec{u})$ one can get an idea of how many samples one needs to do this distinguishment.

For implementation of Hawk, they provide tables which along with an algorithm determines the distribution $\dgdi_{2\bb{Z} + c, \sigma}$. Based on the tables one can reconstruct a 
Probability Mass Function (PMF) $Pr(X = x)$ for $X \sim \dgdi_{\bb{Z}, \sigma$.
    The PMF of $\dgdi_{\bb{Z}, \sigma$ from tables is:
\[
    Pr(X=x) = \frac{1}{2} \cdot
\begin{cases}
    1 - T_c[0] & \text{, if } \lvert x \rvert = c \\
    \frac{1}{2}\mathlarger{(T_c[\frac{\lvert x \rvert - c}{2} - 1] - T_c[\frac{\lvert x \rvert - c}{2}])} & \text{, if } \lvert x \rvert > c
\end{cases}
\]
Here, $c = \lvert x \rvert \mod{2}$ that determines which table ($T_0$ or $T_1$) is used. \todo{Explain this stuff a bit better maybe}

Given the PMF we can compute the moments of $X$ as $\bb{E}[X^k] = \mathlarger{\sum_x x^k \cdot Pr(X = x)}$, as there is a finite number of entries in the tables.
More specifically, for the 256 parameter set, there are 10 entries in $T_0$ and $T_1$. Therefore we sum from $x = -20$ to $x = 20$.
When $|x| = 20$, $Pr(\lvert X \rvert \geq 20) = \frac{1}{4} \cdot (T_0 [9] - T_0[10])$, where $T_0[9]$ is the last non-zero entry, and so all successive entries will yield 0.
Similarly, for parameters 512 and 1024 we sum from $x = -26$ to $x = 26$, since there are 13 entries in $T_0$ and $T_1$.

By using the Decimal module in Python \cite{Decimal}, a module designed for high precision when working with decimal numbers, we can be confident that the computed results are correct, 
and that we do not get e.g. floating point errors when summing, since we are dealing with very small numbers.

We compute $\mu_4$ as $\mu_4 = \bb{E}[Z^4] = \bb{E}[(\frac{X}{\sigma})^4] = \frac{1}{\sigma^4}\bb{E}[X^4]$.
In table \ref{z4 values} we show the value of $\mu_4$ and $\mu_4 - 3$ rounded at 25 and 39 decimal points, respectively, for all three Hawk degrees.

\begin{table}[H]
    \centering
    \caption{Measure of $\mu_4$}
    \label{z4 values}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Degree} & $\bb{E}[Z^4]$ & $\bb{E}[Z^4] - 3$ \\
        \midrule
        256 & 2.9999999999999790015752619 & -2.0998424738064930232320331 $\cdot 10^{-14}$ \\
        \midrule
        512 & 2.9999999999999999999862684 & -1.3731593731486976093589903 $\cdot 10^{-20}$ \\
        \midrule
        1024 & 2.999999999999999999987558 & -1.2441396421557256288414318 $\cdot 10^{-20}$ \\
        \bottomrule
    \end{tabular}
\end{table}



\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{hpp_parallelepiped_normal_discrete.png}
    \caption{Hidden parallelepiped in dimension 2 for discrete Gaussian distribution ($\sigma = 2.02$)}
  	\medskip 
	% \hspace*{15pt}\hbox{\scriptsize Credit: Acme company makes everything \url{https://acme.com/}}
    \label{parallelepiped_normal_discrete}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{hppnormal_cube_discrete.png}
    \caption{Hidden hypercube in dimension 2 for discrete Gaussian distribution ($\sigma = 2.02$)}
  	\medskip 
	% \hspace*{15pt}\hbox{\scriptsize Credit: Acme company makes everything \url{https://acme.com/}}
    \label{hypercube_normal_discrete}
\end{figure}
}

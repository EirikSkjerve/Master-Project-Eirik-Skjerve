\newcommand{\PP}[2][]{\mathcal{P}_{#1}(\mat{#2})}
\newcommand{\mat}[1]{\mathit{#1}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\GLnR}{\mathcal{GL}_{n}(\mathbb{R})}
\newcommand{\normdist}[2]{\mathcal{N}(#1, #2^2)}
\newcommand{\bb}[1]{\mathbb{#1}}

\chapter{Hawk and \textit{Learning a parallelepiped}}

\section{Hawk}
In the following Hawk, the digital signature scheme, will be presented, as in \cite{HawkSpec24}
\subsection{Simple Hawk}
Present simple sketch of keygen, sign and ver, as well as an example in dimension 2
\subsection{Key generation}
\subsection{Signature generation}
\subsection{Signature verification}

\section{Learning a parallelepiped}
The paper \textit{Learning a Parallelepiped: Cryptanalysis of GGH and NTRU Signatures} by Phong Q. Nguyen and Oded Regev from 2006 
introduced a method for breaking digital signature schemes based on the GHH scheme \cite{NR09}. 
Essentially, by observing enough \textit{message, signature} pairs generated by a secret key one can deduce this secret key. 
The attack broke the NTRU-Sign scheme by observing as little as 400 signatures.

\subsection{Solving the Hidden Parallelepiped Problem}
First we define an idealized version of both the problem to solve and the solution as proposed in \cite{NR09}. Then we discuss the problem and solution in a practical context.
\paragraph{Hidden Parallelepiped Problem (HPP):} Let $\mat{V} = [\vec{v}_1, ... \vec{v}_n ]$ be a secret $n \times n$ matrix and $\mat{V} \in \GLnR$.
Define by $\PP{V} = \{\sum_{i=1}^{n} x_i \vec{v}_i : x_i \in [-1, 1]\}$ a secret $n$-dimensional parallelepiped defined by V.
Given a polynomial (in $n$) number of vector samples uniformly distributed over $\PP{V}$, recover the rows $\vec{v}_i$ of $\mat{V}$. \\
We solve this problem in two main steps: 
\begin{enumerate}
    \item Transforming the hidden parallelepiped into a hidden hypercube
    \item Learning the hypercube
\end{enumerate}
\paragraph{Parallelepiped $\rightarrow$ Hypercube}
The first step of the attack is to convert our hidden parallelepiped $\PP{V}$ into a hidden hypercube 
$\PP{C}$ with orthogonal basis vectors.
Essentially, one moves each sampled point in accordance to a transformation matrix $\mat{L}$ computed the following way: \\
\begin{itemize}
    \item Approximate $\mat{G} \approx \mat{V}^T \mat{V}$ using our samples $\mathcal{X} = \{\vec{x}_1, ..., \vec{x}_u\}$
    \item Compute $\mat{L}$ such that $\mat{L} \mat{L}^{T} = \mat{G}^{-1}$
    \item Then $\mat{C} = \mat{V} \mat{L}$
    \item By multiplying our samples $\mathcal{X}$ to the right by $\mat{L}$, they are now uniformly distributed over the hidden hypercube $\PP{C}$
    \item \textbf{(---SHOW CALCULATIONS---)}
\end{itemize}

\paragraph{Learning a Hypercube:}
The second step is to learn the hypercube. Given samples over $\PP{C}$, we deduce the rows of the secret matrix $\mat{C}$ with the method described in Algorithm 1. After the rows are approximated,
one can multiply the rows $\{\vec{c}_1, ..., \vec{c}_2\}$ by $\mat{L}^{-1}$ such that we have $\{\vec{v}_1, ..., \vec{v}_2\}$, and we are done.
\begin{algorithm}
    \caption{Learning a Hypercube}
    \begin{algorithmic}
        \Require Descent parameter $\delta$, samples $\mathcal{X}$ uniformly distributed over $\PP{C}$
        \Ensure A row vector $\pm \vec{v}_i$ of $\mat{C}$
        \State Choose uniformly at random $\vec{w}$ on the unit sphere of $\bb{R}^n$
        \Loop
        \State Compute $\vec{g}$, an approximation of $\nabla mom_{4}(\vec{w})$
        \State Let $\vec{w}_{new} = \vec{w} - \delta \vec{g}$
        \State Place $\vec{w}_{new}$ back on the unit sphere by dividing it by $\left \Vert \vec{w}_{new} \right \Vert$
        \If{$mom_4(\vec{w}_{new}) \ge mom_4(\vec{w})$} \Comment $mom_4$ are approximated by samples 
            \State \Return $\vec{w}$
        \Else
            \State Replace $\vec{w}$ with $\vec{w_{new}}$ and continue loop
        \EndIf
        \EndLoop
    \end{algorithmic}
\end{algorithm}
\hfill \break \\
\subsection{HPP against NTRU}
\subsection{HPP against normally distributed samples}
In the following, we see what happens to the computations the \textit{Learning a parallelepiped} attack is based on if we replace the uniform distribution by a normal distribution.
The key component and assumption of the \textit{Learning a parallelepiped} attack is that the provided samples are distributed uniformly over $\PP{V}$.
Recall that $\PP{V}$ is defined as $\{\sum_{i=1}^n x_i \vec{v}_i : x_i \in [-1, 1]^n\}$ where $\vec{v}_i$ are rows of $\mat{V}$ (generally one can take another interval than $[-1, 1]$ and do appropriate scaling).
One runs into trouble if the sampled vectors are on the form $\vec{v} = \vec{x} \mat{V}$ where $\vec{x}$ follows a normal distribution, i.e. $x_i \sim \normdist{\mu}{\sigma}$.
Although one might be able to approximate the covariance matrix $\mat{V}^t \mat{V}$, one can not do a gradient descent based on the fourth moment given such samples; the fourth moment is constant since the samples form a hypersphere.

\paragraph{Adapting definition of $\PP{V}$}
Firstly, the distribution $\normdist{\mu}{\sigma}$ is defined over the interval $[- \infty, \infty]$, so it does not make sense to talk about samples "normally distributed over $\PP{V}$" without tweaking any definitions.
Therefore, let $[- \eta, \eta]$ be a finite interval on which to consider a truncated normal distribution $\mathcal{N}_{\eta}(\mu, \sigma^2)$ such that $\int_{-\eta}^{\eta} f_X(x) dx = 1 - \delta$ for some negligibly small $\delta$
where $f_X(x)$ is the probability density function of $\normdist{\mu}{\sigma}$.
Now we consider $\PP[\eta]{V} = \{\sum_{i=1}^n x_i \vec{v}_i : x_i \in [-\eta, \eta]^n\}$ and proceed as in the original HPP with $\PP[\eta]{V}$ instead of $\PP{V}$.

\paragraph{Approximating $\mat{V}^t \mat{V}$}
Let $\mat{V} \in \GLnR$. Let $\vec{v}$ be chosen from a truncated normal distribution $\mathcal{N}_{\eta}(0, \sigma^2)$ over $\PP[\eta]{V}$.
Then $\lim_{\eta\to\infty}$ $\bb{E}[\vec{v}^t\vec{v}] = \mat{V}^t \mat{V} \cdot \sigma^2$, where $\sigma^2$ is the variance of the current distribution.

\begin{proof}
    Let samples be on the form $\vec{v} = \vec{x}\mat{V}$, where $\vec{x}$ is a vector where each element $x_i \sim \mathcal{N}_{\eta}(0, \sigma^2)$ over the interval $[-\eta, \eta]$.
    Then $\vec{v}^t\vec{v} = (\vec{x} \mat{V})^t (\vec{x} \mat{V}) = (\mat{V}^t \vec{x}^t)(\vec{x} \mat{V}) = \mat{V}^t \vec{x}^t \vec{x} \mat{V}$. Considering $\bb{E}[\vec{x}^t \vec{x}]$, we see that for $i \neq j$, 
$\bb{E}[x_i x_j] = \bb{E}[x_i] \bb{E}[x_j] = 0 \cdot 0 = 0$ due to independent random variables.
For $i=j$, $\lim_{\eta\to\infty} \bb{E}[x_i^2] = \bb{V}[x_i] = \sigma^2$ since $\bb{V}[x_i] = \bb{E}[x_i^2] - \bb{E}[x_i]^2 = \bb{E}[x_i^2] - 0 = \sigma ^2$.
Therefore, $\lim_{\eta\to\infty} \bb{E}[\vec{x}^t \vec{x}] = \mat{I}_n \cdot \sigma^2$, i.e. the identity matrix with diagonal entries multiplied by $\sigma ^2$.
Consequently, $\lim_{\eta\to\infty} \vec{v}^t \vec{v} = \mat{V}^t \bb{E}[\vec{x}^t\vec{x}] \mat{V} = \mat{V}^t \mat{I}_n \cdot \sigma^2 \mat{V} = (\mat{V}^t \mat{V}) \cdot \sigma ^2$ 
and conversely $\lim_{\eta\to\infty} \mat{V}^t \mat{V} = (\vec{v}^t \vec{v})/ \sigma^2$.
\end{proof}

This means that we can in theory approximate the covariance matrix $\mat{V}^t \mat{V}$ by averaging over $\vec{v}^t \vec{v}$ and dividing by $\sigma ^2$. 
However, it is not immediately clear if one needs more samples for this approximation than in the original attack due to the difference in distributions.
In addition, it is not clear exactly how accurate our approximation needs to be for the remaining parts of the attack to work. 

\paragraph{Hypercube transformation}
Assume now that we know $\mat{V}^t \mat{V}$. Consider instead of $\PP{V}$, $\PP[\eta]{V}$.
Then by following part 1 of \textbf{Lemma 2} and its proof from \cite{NR09} we can transform our hidden parallelepiped $\PP[\eta]{V}$ into $\PP[\eta]{C}$, a hidden hypercube,
since this does not depend on what distribution the samples follow - it only assumes one knows $\mat{V}^t\mat{V}$.
For completeness, by adapting the second part of \textbf{Lemma 2} to our case: 
\begin{proof}
    Let $\vec{v} = \vec{x}\mat{V}$ where $\vec{x}$ is normally distributed according to $\mathcal{N}_{\eta}(0, \sigma^2)$, considering the finite interval $[-\eta,\eta]$.
    It follows then that $\vec{v}\mat{L} = \vec{x}\mat{V}\mat{L} = \vec{x}\mat{C}$ has a truncated uniform distribution over $\PP[\eta]{C}$.
\end{proof}
Thus, we should also be able to map our normally distributed samples onto the hidden hypercube.

\paragraph{Learning a hypercube}
It is clear that samples uniformly over $\PP[\eta]{C}$ centered at the origin will form a hypersphere for which any rotation leaves the sphere practically equal. As a consequence, any measure of the fourth moment of one-dimensional projections is
useless because every projection will yield the same result.
For completeness, we show the results.

Analogous to \cite{NR09} we compute the 2nd and 4th moment of $\PP{V}$ over a vector $\vec{w} \in \bb{R}$: \\

The \textit{k}-th moment of $\PP{V}$ over a vector $\vec{w}$ is defined as $mom_{\mat{V}, k} = \bb{E}[\langle \vec{u}, \vec{w} \rangle ^k]$ where $\vec{u} = \sum_{i=1}^{n} x_i \vec{v}_i$ and $x_i \sim \mathcal{N}_{\eta}(0, \sigma^2)$.
First we consider $\langle \vec{u}, \vec{w} \rangle = \langle \sum_{i=1}^{n} x_i\vec{v}_i, \vec{w}\rangle = \sum_{i=1}^{n}x_i \langle\vec{v}_i, \vec{w} \rangle$.
Then for $k=2$, $\bb{E}[(\sum_{i=1}^{n}x_i \langle\vec{v}_i, \vec{w} \rangle)^2] = \bb{E}[\sum_{i=1}^{n} \sum_{j=1}^{n} x_i x_j \langle \vec{v}_i, \vec{w} \rangle \langle \vec{v}_j \vec{w} \rangle]$.
Due to independent random variables, $x_i x_j = 0$ when $i \neq j$, so we have $\bb{E}[\sum_{i=1}^{n}x_i ^2 \langle \vec{v}_i, \vec{w} \rangle ^2] = \sum_{i=1}^{n}\bb{E}[x_i^2]\langle \vec{v}_i, \vec{w} \rangle^2$ 
$= \sigma^2 \sum_{i=1}^{n}\langle \vec{v}_i, \vec{w} \rangle^2$.
Thus, we end up with:
\begin{equation}
mom_{\mat{V}, 2}(\vec{w}) = \sigma^2 \vec{w}\mat{V}^t\mat{V}\vec{w}^t \\
\end{equation}

For $k=4$, inside the $\bb{E}[ \ ]$ we have 
\[
    (\sum_{i=1}^{n} x_i \langle \vec{v}_i, \vec{w} \rangle)^4 = \sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n}\sum_{l=1}^{n}x_i x_j x_k x_l \langle \vec{v}_i, \vec{w} \rangle \langle \vec{v}_j, \vec{w} \rangle \langle \vec{v}_k, \vec{w} \rangle \langle \vec{v}_l, \vec{w} \rangle
\]
We consider three cases for the indices $i, j, k,$ and $l$:
\begin{enumerate}
    \item \textbf{All equal}: if $i = j = k = l$, then we have $\sum_{1=1}^{n} \bb{E}[x_i ^4] \langle \vec{v}_i, \vec{w} \rangle^4$. A well known result for the normal distribution $\normdist{0}{\sigma}$ is that $\bb{E}[x^4] = 3 \sigma^4$.
        
\end{enumerate}

\begin{equation}
mom_{\mat{V}, 4}(\vec{w}) = 3 \sigma^4 (\sum_{i=1}^{n} <v_i, \vec{w}>^4 + \sum_{i \neq j} <v_i, \vec{w}>^2 <v_j, \vec{w}>^2)
\end{equation}
It turns out that if $\mat{V} \in \mathcal{O}(\bb{R})$, i.e. our samples are over a hypercube and $\vec{w}$ is on the unit sphere the 4th moment is constant. We show this by rewriting (3.2) as  
\[ mom_{\mat{V}, 4}(\vec{w}) = 3\sigma^4(\sum_{i=1}^n<v_i, \vec{w}>^4 + \sum_{i=1}^n <v_i, \vec{w}>^2 \sum_{j=1}^n <v_j, \vec{w}>^2 - \sum_{i=1}^n<v_i, \vec{w}>^4 )\]
\[ = 3\sigma^4(\sum_{i=1}^n <v_i, \vec{w}>^2 \sum_{j=1}^n <v_j, \vec{w}>^2) = 3\sigma^4(\sigma^2\left \Vert \vec{w} \right \Vert^2)^2 = 3\sigma^8\]
because $mom_{\mat{V}, 2}(\vec{w}) = \sum_{i=1}^n <v_i, \vec{w}>^2 = \sigma^2 \left \Vert \vec{w} \right \Vert^2$ when $\mat{V} \in \mathcal{O}(\bb{R})$ and $\left \Vert \vec{w} \right \Vert^2 = 1$ when $\vec{w}$ lies on the unit sphere.

In conclusion, if samples over the secret parallellepiped $\PP{V}$ follow a continuous normal distribution, the covariance matrix approximation requires many samples, and a gradient descent based on the fourth moment described in \cite{NR09}
is impossible because the fourth moment is constant over the unit sphere of $\bb{R}^n$.

\paragraph{What about a discrete Gaussian distribution?}
We now do the same computations considering the discrete Gaussian distribution as described in section 2 and \cite{HawkSpec24}. It turns out that this distribution behaves like the Normal distribution, and consequently gradient descent
is impossible.

% As a countermeasure against these types of attacks, it was proposed in \cite{GPV07} to not use the uniform distribution when creating signatures. Rather, by sampling from a Discrete Gaussian distribution
% (a discrete analogue to the Normal distribution), signatures do not "leak" any information about the secret parallelepiped, and the geometry of the key remains concealed.

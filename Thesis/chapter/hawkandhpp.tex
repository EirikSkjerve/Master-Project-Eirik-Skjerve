\chapter{Hawk and \textit{Learning a parallelepiped}}

\section{Hawk}
In the following Hawk, the digital signature scheme, will be presented, as in \cite{HawkSpec24}
\subsection{Simple Hawk}
Present simple sketch of keygen, sign and ver, as well as an example in dimension 2
\subsection{Key generation}
\subsection{Signature generation}
\subsection{Signature verification}

\section{Learning a parallelepiped}
The paper \textit{Learning a Parallelepiped: Cryptanalysis of GGH and NTRU Signatures} by Phong Q. Nguyen and Oded Regev from 2006 
introduced a method for breaking digital signature schemes based on the GHH scheme \cite{NR09}. 
Essentially, by observing enough \textit{message, signature} pairs generated by a secret key one can deduce this secret key. 
The attack broke the NTRU-Sign scheme by observing as little as 400 signatures.

\subsection{Solving the Hidden Parallelepiped Problem}
First we define an idealized version of both the problem to solve and the solution as proposed in \cite{NR09}. Then we discuss the problem and solution in a practical context.
\paragraph{Hidden Parallelepiped Problem (HPP):} Let $\mathit{V} = [\mathbf{v}_1, ... \mathbf{v}_n ]$ be a secret $n \times n$ matrix and $\mathit{V} \in \mathcal{GL}_n (\mathbb{R})$.
Define by $\mathcal{P}(\mathit{V}) = \{\sum_{i=1}^{n} x_i \mathbf{v}_i : x_i \in [-1, 1]\}$ a secret $n$-dimensional parallelepiped defined by V.
Given a polynomial (in $n$) number of vector samples uniformly distributed over $\mathcal{P}(\mathit{V})$, recover the rows $\mathbf{v}_i$ of $\mathit{V}$. \\
We solve this problem in two main steps: 
\begin{enumerate}
    \item Transforming the hidden parallelepiped into a hidden hypercube:
    \item Learning the hypercube:
\end{enumerate}
\paragraph{Parallelepiped $\rightarrow$ Hypercube}
The first step of the attack is to convert our hidden parallelepiped $\mathcal{P}(\mathit{V})$ into a hidden hypercube 
$\mathcal{P}(\mathit{C})$ with orthogonal basis vectors.
Essentially, one moves each sampled point in accordance to a transformation matrix $\mathit{L}$ computed the following way: \\
\begin{itemize}
    \item Approximate $\mathit{G} \approx \mathit{V}^T \mathit{V}$ using our samples $\mathcal{X} = \{\mathbf{x}_1, ..., \mathbf{x}_u\}$
    \item Compute $\mathit{L}$ such that $\mathit{L} \mathit{L}^{T} = \mathit{G}^{-1}$
    \item Then $\mathit{C} = \mathit{V} \mathit{L}$
    \item By multiplying our samples $\mathcal{X}$ to the right by $\mathit{L}$, they are now uniformly distributed over the hidden hypercube $\mathcal{P}(\mathit{C})$
    \item \textbf{(---SHOW CALCULATIONS---)}
\end{itemize}

\paragraph{Learning a Hypercube:}
The second step is to learn the hypercube. Given samples over $\mathcal{P}(\mathit{C})$, we deduce the rows of the secret matrix $\mathit{C}$ with the method described in Algorithm 1. After the rows are approximated,
one can multiply the rows $\{\mathbf{c}_1, ..., \mathbf{c}_2\}$ by $\mathit{L}^{-1}$ such that we have $\{\mathbf{v}_1, ..., \mathbf{v}_2\}$, and we are done.
\begin{algorithm}
    \caption{Learning a Hypercube}
    \begin{algorithmic}
        \Require Descent parameter $\delta$, samples $\mathcal{X}$ uniformly distributed over $\mathcal{P}(\mathit{C})$
        \Ensure A row vector $\pm \mathbf{v}_i$ of $\mathit{C}$
        \State Choose uniformly at random $\mathbf{w}$ on the unit sphere of $\mathbb{R}^n$
        \Loop
        \State Compute $\mathbf{g}$, an approximation of $\nabla mom_{4}(\mathbf{w})$
        \State Let $\mathbf{w}_{new} = \mathbf{w} - \delta \mathbf{g}$
        \State Place $\mathbf{w}_{new}$ back on the unit sphere by dividing it by $\left \Vert \mathbf{w}_{new} \right \Vert$
        \If{$mom_4(\mathbf{w}_{new}) \ge mom_4(\mathbf{w})$} \Comment $mom_4$ are approximated by samples 
            \State \Return $\mathbf{w}$
        \Else
            \State Replace $\mathbf{w}$ with $\mathbf{w_{new}}$ and continue loop
        \EndIf
        \EndLoop
    \end{algorithmic}
\end{algorithm}
\hfill \break \\
\subsection{HPP against NTRU}
\subsection{HPP against normally distributed samples}
The key component and assumption of the \textit{Learning a parallelepiped} attack is that the provided samples are distributed \textit{uniformly} over $\mathcal{P}(\mathit{V})$. 
Assume now that samples over $\mathcal{P}(\mathit{V})$ is distributed normally.

\paragraph{Approximating $\mathit{V}^t \mathit{V}$}
Let $\mathit{V} \in \mathcal{GL}_n(\mathbb{R})$. Let $\mathbf{v}$ be chosen from a normal distribution over $\mathcal{P}(\mathit{V})$. 
Then $\mathbb{E}[\mathbf{v}^t\mathbf{v}] = \mathit{V}^t \mathit{V}/\sigma^2$.\\

\begin{proof}
Let $\mathbf{v} = \mathbf{x}\mathit{V}$ where $\mathbf{x}$ is distributed over $\mathcal{N}(\mu = 0,\sigma)$, over the interval $[-\infty, \infty]$.
Then $\mathbf{v}^t\mathbf{v} = \mathit{V}^t \mathbf{x}^t \mathbf{x} \mathit{V}$. Considering $\mathbb{E}[\mathbf{x}^t \mathbf{x}]$, we see that for $i \neq j$, 
$\mathbb{E}[x_i x_j] = \mathbb{E}[x_i] \mathbb{E}[x_j] = 0 \cdot 0 = 0$ due to independent random variables.
For $\mathbb{E}[x_i^2] = \mathbb{V}[x_i]$ since $\mathbb{V}[x_i] = \mathbb{E}[x_i^2] - \mathbb{E}[x_i]^2 = \mathbb{E}[x_i^2] - 0 = \sigma ^2$.
Therefore, $\mathbb{E}[\mathbf{x}^t \mathbf{x}] = \frac{\mathit{I}_n}{\sigma^2}$, i.e. the identity matrix with diagonal entries divided by $\sigma ^2$.
Consequently, $\mathbf{v}^t \mathbf{v} = \mathit{V}^t \mathbb{E}[\mathbf{x}^t\mathbf{x}] \mathit{V} = \mathit{V}^t \frac{\mathit{I}_n}{\sigma^2} \mathit{V} = \mathit{V}^t \mathit{V} / \sigma ^2$ 
and conversely $\mathit{V}^t \mathit{V} = \sigma^2 \cdot \mathbf{v}^t \mathbf{v}$.
\end{proof}

This means that we can in theory approximate the covariance matrix $\mathit{V}^t \mathit{V}$. However, it is clear that when the distribution is normal instead of uniform, one needs many more samples (why is this?).
The question is also the following: how accurate does our approximation need to be?

% As a countermeasure against these types of attacks, it was proposed in \cite{GPV07} to not use the uniform distribution when creating signatures. Rather, by sampling from a Discrete Gaussian distribution
% (a discrete analogue to the Normal distribution), signatures do not "leak" any information about the secret parallelepiped, and the geometry of the key remains concealed.

\chapter{Hawk and \textit{Learning a parallelepiped}}

\section{Hawk}
In the following Hawk, the digital signature scheme, will be presented, as in \cite{hawkspec}
\subsection{Simple Hawk}
Present simple sketch of keygen, sign and ver, as well as an example in dimension 2
\subsection{Key generation}
\subsection{Signature generation}
\subsection{Signature verification}

\section{Learning a parallelepiped}
The paper \textit{Learning a Parallelepiped: Cryptanalysis of GGH and NTRU Signatures} by Phong Q. Nguyen and Oded Regev from 2006 
introduced a method for breaking digital signature schemes based on the GHH scheme \cite{hpp}. 
Essentially, by observing enough \textit{message, signature} pairs generated by a secret key one can deduce this secret key. 
The attack broke the NTRU-Sign scheme by observing as little as 400 signatures.

% \subsection{Assumptions}
% By collecting enough signatures on the form $\mathbf{s} = \nint{\textbf{m} \mathbf{B}^{-1}}\mathbf{B}$
% where $\mathbf{m}$ is a hash of some message and $\mathbf{B}$ is the secret basis, one can recover $\mathbf{B}$. 
% First we look at an idealized case to get some understanding of the Hidden Parallelepiped Problem:
% Let $\mathbf{B}$ be a secret $n \times n$ matrix. Let $\{\mathbf{s}_1, \mathbf{s}_2, ... \mathbf{s}_p\}$ where $\mathbf{s}_i = \nint{\textbf{m}_i \mathbf{B}^{-1}}\mathbf{B}$
% and $\mathbf{m}_i$ is uniformly distributed over some interval $[0, q]$ be $p$ signatures on "random" messages.
\subsection{Solving the Hidden Parallelepiped Problem}
First we define an idealized version of both the problem to solve and the solution as proposed in \cite{hpp}. Then we discuss the problem and solution in a practical context.
\paragraph{Hidden Parallelepiped Problem (HPP):} Let $\mathit{V} = [\mathbf{v}_1, ... \mathbf{v}_n ]$ be a secret $n \times n$ matrix and $\mathit{V} \in \mathcal{GL}_n (\mathbb{R})$.
Define by $\mathcal{P}(\mathit{V}) = \{\sum_{i=1}^{n} x_i \mathbf{v}_i : x_i \in [-1, 1]\}$ a secret $n$-dimensional parallelepiped defined by V.
Given a polynomial (in $n$) number of vector samples uniformly distributed over $\mathcal{P}(\mathit{V})$, recover the rows $\mathbf{v}_i$ of $\mathit{V}$. \\
We solve this problem in two main steps: 
\begin{enumerate}
    \item Transforming the hidden parallelepiped into a hidden hypercube:
    \item Learning the hypercube:
\end{enumerate}
\paragraph{Parallelepiped $\rightarrow$ Hypercube}
The first step of the attack is to convert our hidden parallelepiped $\mathcal{P}(\mathit{V})$ into a hidden hypercube $\mathcal{P}(\mathit{C})$ with orthogonal basis vectors.
Essentially, one moves each sampled point in accordance to a transformation matrix $\mathit{L}$ computed the following way: \\
\begin{itemize}
    \item Approximate $\mathit{G} \approx \mathit{V}^T \mathit{V}$ using our samples $\mathcal{X} = \{\mathbf{x}_1, ..., \mathbf{x}_u\}$
    \item Compute $\mathit{L}$ such that $\mathit{L} \mathit{L}^{T} = \mathit{G}^{-1}$
    \item Then $\mathit{C} = \mathit{V} \mathit{L}$
    \item By multiplying our samples $\mathcal{X}$ to the left by $\mathit{L}$, our samples are now uniformly distributed over the hidden hypercube $\mathcal{P}(\mathit{C})$
\end{itemize}

\paragraph{Learning a Hypercube}
ABCDEFG
\subsection{Covariance matrix}
\subsection{Hidden parallelepiped to hidden hypercube}
\subsection{Gradient descent}
\subsection{Example in dimension 2}
\subsection{Hawk resistance against HPP}

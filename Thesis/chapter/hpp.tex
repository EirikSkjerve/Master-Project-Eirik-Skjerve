\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}
\newtheorem{hpp_problem_basic}{Problem}

\section{HPP}
In this section we present the \textit{Learning a Parallelepiped} attack as described in \cite{NR09}. 
Note that in this section bold lowercase $\vec{v}$ denotes a \textit{row}-vector to follow the same notation as in the original paper.
\subsection{Setup and idealized version}
Let $\mat{V} \in \mathcal{GL}(\bb{R})$ be a secret $n \times n$ unimodular matrix and $\PP{V}$ be a fundamental parallelepiped, defined as the set $\{\vec{x}\mat{V}: \vec{x} \in [-1, 1]^n\}$. 
Let $\vec{v} = \vec{x} \mat{V}$. 
The problem to solve is informally described below and visualized in figure \ref{hpp_parallelepiped_uniform}.

\begin{hpp_problem_basic}\label{Basic HPP problem}
    Given enough vectors $\vec{v} = \vec{x}\mat{V}$ (i.e. by observing a large enough subset of $\PP{V}$), where both $\vec{x}$ and $\mat{V}$ is unknown,
recover rows of $\pm \mat{V}$
\end{hpp_problem_basic}
The following three steps are used to solve Problem \ref{Basic HPP problem}:
\begin{enumerate}
    \item Estimate the covariance matrix $\mat{V}^t \mat{V}$
    \item Transform samples $\vec{v} \in \PP{V}$ to $\vec{c} \in \PP{C}$ where $\PP{C}$ is a hypercube, i.e. $\mat{C} \mat{C}^t = \mat{I}_n$
    \item Do gradient descent to minimize the fourth moment of one-dimensional projections and reveal a row of $\pm \mat{C}$ which finally can be transformed into a row of $\pm \mat{V}$
\end{enumerate}
In the following, each of these steps will be covered in detail.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{hpp_parallelepiped_uniform.png}
    \caption{Hidden parallelepiped problem in dimension 2}
  	\medskip 
	% \hspace*{15pt}\hbox{\scriptsize Credit: Acme company makes everything \url{https://acme.com/}}
    \label{hpp_parallelepiped_uniform}
\end{figure}
\subsection{Covariance matrix estimation}
Given enough samples on the form $\vec{v} = \vec{x}\mat{V} $, we want to estimate the covariance matrix $\mat{G} \approx \mat{V}^t \mat{V}$.
This is achieved as $\vec{v}^t \vec{v} = ( \vec{x}\mat{V})^t(\vec{x}\mat{V}) = \mat{V}^t \vec{x}^t \vec{x} \mat{V}$.
% Remark that since $\vec{x}$ is a row vector, $\bb{E}[\vec{x}^t \vec{x}]$ will be an $n \times n$ matrix.
Now, by taking the expectation of the inner term we get $\bb{E}[\vec{x}^t \vec{x}] = \mat{I}_n / 3$ where $\mat{I}_n$ is the $n \times n$ identity matrix because of the following:
Since all $x_i \in \vec{x}$ are distributed according to the uniform distribution over $[-1, 1]$ and each $x_i$ are independent, 
we have that $\bb{E}[x_i] = 0$ and $\bb{E}[x_i x_j] = \bb{E}[x_i] \bb{E}[x_j] = 0$ when $i \neq j$.
\todo{Rewrite this computation: not quite correct notation for integration}
For the case when $i = j$, we have \[\bb{E}[x_i x_j] = \bb{E}[x ^2] = \int_{a}^{b} x^2 \frac{1}{b-a} dx = \int_{-1}^{1} x^2 \frac{1}{2} dx = \frac{1}{3}\]
Therefore, $\bb{E}[\vec{x}^t \vec{x}]$ is an $n \times n$ matrix with $\frac{1}{3}$ down the diagonal and is 0 otherwise, i.e. $\mat{I}_n / 3$.
Thus, as number of samples grow, $\vec{v} ^t \vec{v} \rightarrow \mat{V}^t (\mat{I}_n / 3) \mat{V}$, and therefore $\vec{v}^t \vec{v} \cdot 3 \rightarrow \mat{V}^t \mat{V}$.
In conclusion: by taking the average of $\vec{v}^t \vec{v}$ for all collected samples, and multiplying the resulting $n \times n$ matrix with $3$, one 
has a good approximation of the covariance matrix $\vec{V}^t \vec{V}$.
% Clearly, the more samples $\vec{v}$ one has, the more accurate the approximation $\mat{G} \approx \mat{V} ^t \mat{V}$ is.
\subsection{Hidden parallelepiped to hidden hypercube transformation}
Given a good approximation $\mat{G} \approx \mat{V}^t \mat{V}$, the next step is to calculate a linear transformation $\mat{L}$ such that the following is true:
\begin{enumerate}
    \item $\mat{C} = \mat{V}\mat{L}$ is orthonormal, i.e. the rows are pairwise orthogonal and the norm of each row is 1. In other words, $\mat{C} \mat{C}^t = \mat{I}$. 
        Consequently, $\PP{C}$ becomes a hypercube.
    \item If $\vec{v}$ is uniformly distributed over $\PP{V}$ then $\vec{c} = \vec{v} \mat{L}$ is uniformly distributed over $\PP{C}$.
\end{enumerate}
% This transformation is visualized from figure \ref{hpp_parallelepiped_uniform} to figure \ref{hpp_uniform_cube}.
This is achieved by taking the Cholesky decomposition of $\mat{G}^{-1} = \mat{L} \mat{L}^t$ where $\mat{L}$ is a lower-triangular matrix. 
To compute Cholesky decomposition of $\mat{G}^{-1}$, we must first show that $\mat{G}$ is symmetric positive definite.
\begin{enumerate}
    \item $\mat{G}$ is symmetric $\iff \mat{G}^{t}= \mat{G}$ which is clear as $\mat{G}^t = (\mat{V}^t\mat{V})^t = \mat{V}^t (\mat{V}^{t})^t= \mat{V}^t \mat{V} = \mat{G}$.
    \item $\mat{G}$ is positive definite if for any non-zero column vector $\vec{x} \in \bb{R}^n$, $\vec{x}^t \mat{G} \vec{x} > 0.$
We have that $\vec{x}^t \mat{G} \vec{x} = \vec{x}^t \mat{V}^t \mat{V} \vec{x} = (\mat{V} \vec{x})^t (\mat{V} \vec{x})$. Denote by $\vec{y} = \mat{V} \vec{x}$.
Since $\vec{x} \neq \mathbf{0}$ and $\mat{V}$ is invertible (and therefore non-zero) it is clear that 
$\vec{y} \neq \mathbf{0}$ and $\vec{y}^t \vec{y} = \lvert \vert{\vec{y}} \rvert \vert ^2 > 0$.
\end{enumerate}
Since $\mat{G}$ is symmetric positive definite we can do decomposition to get $\mat{L}$. Then: 
\begin{enumerate}
    \item if $\mat{C} = \mat{V} \mat{L}$, then $\mat{C} \mat{C}^t = \mat{V} \mat{L} \mat{L}^t \mat{V}^t = \mat{V} \mat{G}^{-1} \mat{V}^t = \mat{V} (\mat{V^t}\mat{V})^{-1} \mat{V}^t
        = \mat{V} \mat{V}^{-1} \mat{V}^{-t} \mat{V}^t = \mat{I}$
    \item since entries in $\vec{x}$ are uniformly distributed, $\vec{c} = \mat{C} \vec{x}$ is uniformly distributed over $\PP{C}$
\end{enumerate}

By multiplying our samples $\vec{v}$ by $\mat{L}$ on the right, we transform them from the hidden parallelepiped to the hidden hypercube. 
Therefore, the problem to solve now can be called the "Hidden Hypercube Problem".
If one finds the rows of $\pm \mat{C}$, one can simply multiply the result on the right by $\mat{L}^{-1}$ to obtain the solution for $\mat{V}$.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{hpp_uniform_cube.png}
    \caption{Hidden hypercube problem in dimension 2}
  	\medskip 
	% \hspace*{15pt}\hbox{\scriptsize Credit: Acme company makes everything \url{https://acme.com/}}
    \label{hpp_uniform_cube}
\end{figure}
\subsection{Moments and Gradient Descent}
The last major step in the attack is to measure and minimize the fourth moment of one-dimensional projections to disclose rows of $\pm \mat{C}$.
Let $mom_{k, \mat{C}}(\vec{w})$ be defined as the $k$-th moment of $\PP{C}$ projected onto $\vec{w}$, i.e. 
$\bb{E}[\langle \vec{c}, \vec{w} \rangle ^k]$ where $\vec{c} = \vec{x} \mat{C}$ for $\vec{x}$ uniformly distributed and $\vec{w} \in \bb{R}^n$.
Looking at the term $\langle \vec{c}, \vec{w} \rangle$, we have $\langle \vec{x} \mat{C}, \vec{w} \rangle = \langle \sum_{i=1}^{n} x_i c_i, \vec{w} \rangle$
where $c_i$ is the $i$-th row of $\mat{C}$. Since $x_i$ is a scalar, we can move it out of the dot-product brackets. $\sum_{i=1}^{n} x_i \langle c_i, \vec{w} \rangle $ .
Evaluating this inside the $\bb{E}[ \ ]$ we have $\bb{E}[\sum_{i=1}^{n} x_i \langle c_i, \vec{w} \rangle ^k ]$.
We look at the two cases when $k = 2$ and $k=4$.
\begin{itemize}
    \item $\mathbf{k = 2: }$
        $\bb{E}[(\sum_{i=1}^{n} x_i \langle c_i, \vec{w} \rangle ) ^2] = \bb{E}[\sum_{i}^{n} \sum_{j}^{n} x_i x_j \langle c_i, \vec{w} \rangle \langle c_j, \vec{w} \rangle]$.
        As seen before in section 3.3.2, $\bb{E}[x_i x_j] = \frac{1}{3}$ when $i = j$ and $0$ otherwise. Thus, we have the final expression 
        $mom_{2, C}(\vec{w}) = \frac{1}{3} \sum_{i}^{n} \langle c_i, \vec{w} \rangle ^2$ which can also be written as
        $ = \frac{1}{3} \vec{w} \mat{C}^t \mat{C} \vec{w}^t$
    \item $\mathbf{k = 4: }$
        \[ \bb{E}[(\sum_{i=1}^{n} x_i \langle c_i, \vec{w} \rangle ) ^4] = 
        \bb{E}[\sum_{i}^{n} \sum_{j}^{n} \sum_{k}^{n} \sum_{l}^{n} x_i x_j x_k x_l 
        \langle c_i, \vec{w} \rangle \langle c_j, \vec{w} \rangle
    \langle c_k, \vec{w} \rangle \langle c_l, \vec{w} \rangle] \]
    There are three cases for the indices $i, j, k$ and $l$:
    \begin{enumerate}
        \item \textbf{All equal: } If $i = j = k = l$, we simply have $\sum_i^n \bb{E}[x^4] \langle c_i, \vec{w} \rangle ^4 = \frac{1}{5} \sum_i \langle c_i, \vec{w} \rangle ^4$
            due to the fact that $\bb{E}[x ^4] = \int_{-1}^{1} x^4 \frac{1}{2} dx = \frac{1}{5}$
        \item \textbf{None equal:} If $i \neq j \neq k \neq l$ the expression is zero due to $\bb{E}[x_i] = 0$ and all $x_i, x_j, x_k, x_l$ are independent.
        \item \textbf{Pairwise equal:} If either 
            \begin{itemize}
                \item $i = j \neq k = l$
                \item $i = k \neq j = l$ 
                \item $i = l \neq j = k$
            \end{itemize}
            then we have 
            $\sum_{i \neq j} \bb{E}[x_i ^2 x_j^2] \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle^2$ 
            $ = \frac{1}{9} \sum_{i \neq j} \langle c_i, \vec{w} \rangle^2 \langle c_j, \vec{w} \rangle^2$. By putting the above together we get
            \[\frac{1}{5} \sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^4 + 3(\frac{1}{9} \sum_{i \neq j} \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2)\]
            and the final expression becomes 
            \[ mom_{4, \mat{C}} (\vec{w}) = \frac{1}{5} \sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^4 + \frac{1}{3} \sum_{i \neq j} \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2\]
    \end{enumerate}

    Now, since $\mat{C}$ is orthonormal, and by restricting $\vec{w}$ to the unit sphere in $\bb{R}^n$, we can simplify the expressions further.
    The second moment becomes $mom_{2, C}(\vec{w}) = \frac{1}{3} \vec{w} \mat{C}^t \mat{C} \vec{w}^t = \frac{1}{3}\vec{w} \mat{I} \vec{w}^t 
    = \frac{1}{3} \vec{w}\vec{w}^t = \frac{1}{3} \lVert \vec{w} \rVert ^2 = \frac{1}{3}$. \\
    By rewriting and expanding the fourth moment:
    \[ mom_{4, \mat{C}} (\vec{w}) = \frac{1}{5} \sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^4 + \frac{1}{3} \sum_{i \neq j} \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2\]
    \[ mom_{4, \mat{C}} (\vec{w}) = \frac{1}{5} \sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^4 + 
        \frac{1}{3} \sum_{i=1}^{n} \sum_{j=1}^n \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2 -
    \frac{1}{3}\sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^2 \langle c_i, \vec{w} \rangle ^2\]
    \[ mom_{4, \mat{C}} (\vec{w}) = \frac{1}{5} \sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^4 + 
    \frac{1}{3} \sum_{i=1}^n \sum_{j=1}^n \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2 -
\frac{1}{3}\sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^4 \]
    \[ mom_{4, \mat{C}} (\vec{w}) = \frac{1}{3} \lVert \vec{w} \rVert ^4 - 
        \frac{2}{15}\sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^4 = \frac{1}{3} - 
    \frac{2}{15}\sum_{i=1}^n \langle c_i, \vec{w} \rangle ^4\]

\end{itemize}

We also need to compute the gradient of the fourth moment, $\nabla mom_{4, \mat{C}} (\vec{w})$.
\begin{itemize}
    \item The gradient of the first term, $\frac{1}{3} \lVert \vec{w} \rVert ^4$, is computed as follows: \\
        We rewrite $\lVert \vec{w} \rVert ^4$ as $(\vec{w}\vec{w}^t)^2$. Using the chain rule we have that \\
        $\nabla ((\vec{w}\vec{w}^t)^2) = 2(\vec{w}\vec{w}^t) \cdot \frac{\partial}{\partial \vec{w}_j}(\vec{w}\vec{w}^t)$
        $= 2(\vec{w}\vec{w}^t) \cdot 2 \vec{w}$. \\ 
        The gradient of the first term is then $\frac{1}{3} \cdot 2(\vec{w}\vec{w}^t) \cdot 2 \vec{w} = \frac{4}{3} \lVert \vec{w} \rVert ^2 \vec{w}$.
    \item For the second term, $\frac{2}{15} \mathlarger{\sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^4}$ we have the following:
        \[\nabla \sum_{i=1}^{n}\langle c_i, \vec{w} \rangle ^4 = \sum_{i=1}^{n}\nabla (\langle c_i, \vec{w} \rangle ^4)\]
        Looking at just the inner term, $\nabla (\langle c_i, \vec{w} \rangle ^4) = 4 \langle c_i, \vec{w} \rangle ^3 
        \cdot \frac{\partial}{\partial w_j} (\langle c_i, \vec{w} \rangle) = 4 \langle c_i, \vec{w} \rangle ^3 \cdot c_i$
        The gradient of the second term is therefore $\mathlarger{\frac{8}{15} \sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^3 c_i}$
\end{itemize}
Putting together the terms we get
\[ \nabla mom_{4, \mat{C}}(\vec{w}) =  \frac{4}{3} \lVert \vec{w} \rVert ^2 \vec{w} - \frac{8}{15} \sum_{i=1}^{n} \langle c_i, \vec{w} \rangle ^3 c_i\] \\
The key observation is that by minimizing $mom_{4, \mat{C}(\vec{w})}$ one has to maximize the term 
$ \mathlarger{-\frac{2}{15}\sum_{i=1}^{n}} \langle c_i, \vec{w} \rangle ^4$. Since $c_i$ and $\vec{w}$ are both on the unit circle, and
all $c_i$ are orthogonal to each other, the term is maximized whenever $\vec{w} = c_i$ since $\langle c_i, \pm c_i \rangle = \langle \vec{w}, \pm \vec{w} \rangle = 1$.
Using this observation, we can run a gradient descent to minimize $mom_{4, \mat{C}}(\vec{w})$ and find rows of $\pm \mat{C}$.
A simple gradient descent is described in the following algorithm:

\begin{algorithm}
\caption{Gradient descent}
\begin{algorithmic}[1]
% \Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
    \Require{Parameter $\delta$, samples $\vec{c}$}
        \State Choose random vector $\vec{w}$ on the unit sphere
        \State Compute an approximation $\vec{g} \gets \nabla mom_{4, \mat{C}}(\vec{w})$
        \State Set $\vec{w}_{new} \gets \vec{w} - \delta \vec{g} $
        \State Normalize $\vec{w}_{new}$ as $\frac{\vec{w}_{new}}{\lVert \vec{w}_{new} \rVert}$
        \State Approximate $mom_{4, \mat{C}}(\vec{w})$ and $mom_{4, \mat{C}}(\vec{w}_{new})$
        \If{$mom_{4, \mat{C}}(\vec{w}) \geq mom_{4, \mat{C}}(\vec{w}_{new})$}
            \State Return $\vec{w}$
        \Else{}
        \State Set $\vec{w} \gets \vec{w}_{new}$ and go to step 2
        \EndIf
\end{algorithmic}
\end{algorithm}
This procedure is repeated until all rows of $\pm \mat{C}$ is found. Each row is transformed by $\mat{L}^{-1}$ to get the row in $ \pm \mat{V}$.
\todo{Should here acknowledge the computational impossibility of computing $n!$ possible candidates}
\section{HPP against the Normal Distribution}

Assume now that the signatures on the form $\vec{v} = \vec{x} \mat{V}$ where $x_i \sim \normdist{0}{\sigma}$. After converting $\PP{V}$ to $\PP{C}$, 
the fourth moment of $\PP{C}$ is constant over $\vec{w}$ on the unit circle.
    To show this, we simply do the same calculations as in the previous section, with the difference that $x$ follows a normal distribution.
    Considering
    \[ mom_{4, \mat{C}}(\vec{w}) = \bb{E}[(\sum_{i=1}^{n} x_i \langle c_i, \vec{w} \rangle ) ^4] = 
        \bb{E}[\sum_{i}^{n} \sum_{j}^{n} \sum_{k}^{n} \sum_{l}^{n} x_i x_j x_k x_l 
        \langle c_i, \vec{w} \rangle \langle c_j, \vec{w} \rangle
    \langle c_k, \vec{w} \rangle \langle c_l, \vec{w} \rangle] \]

    for the three different cases:
    \begin{itemize}
        \item \textbf{All equal: } If $i = j = k = l$, we simply have $\sum_i^n \bb{E}[x_i^4] \langle c_i, \vec{w} \rangle ^4 = 3 \sigma ^4 \sum_i \langle c_i, \vec{w} \rangle ^4$
            due to the well known fact that $\bb{E}[x^4] = 3 \sigma ^4$ for $x$ distributed according to $\normdist{0}{\sigma}$.
        \item \textbf{None equal:} Since $\bb{E}[x] = 0$ as in the case of the uniform distribution, this results in zero.
        \item \textbf{Pairwise equal:} If either 
            \begin{itemize}
                \item $i = j \neq k = l$
                \item $i = k \neq j = l$ 
                \item $i = l \neq j = k$
            \end{itemize}
            we have $\sum_{i \neq j} \bb{E}[x_i^2 x_j^2] \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2$
            $= \sigma^4 \sum_{i \neq j}\langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2$ since $\bb{E}[x^2] = \sigma^2$.
    \end{itemize}

    Putting together expressions we get 
    \[
        mom_{4, \mat{C}}(\vec{w}) = 3 \sigma ^4 \sum_i \langle c_i, \vec{w} \rangle ^4 + 3(\sigma^4 \sum_{i \neq j}\langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2)
    \]

    \[
        mom_{4, \mat{C}}(\vec{w}) = 3 \sigma ^4 \sum_i \langle c_i, \vec{w} \rangle ^4 
        + 3\sigma^4 \sum_{i} \sum_{j} \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2
        - 3\sigma^4 \sum_{i} \langle c_i, \vec{w} \rangle ^4
    \]

    \[
        mom_{4, \mat{C}}(\vec{w}) = 3\sigma^4 \sum_{i} \sum_{j} \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2
    \]

    % \[
    %     mom_{4, \mat{C}}(\vec{w}) = 3\sigma^4 \sum_{i} \sum_{j} \langle c_i, \vec{w} \rangle ^2 \langle c_j, \vec{w} \rangle ^2
    % \]

    \[
        mom_{4, \mat{C}}(\vec{w}) = 3\sigma^4 (\lVert \vec{w} \rVert ^2 ) ^2 = 3 \sigma ^4
    \]

    Since $mom_{4, \mat{C}}(\vec{w})$ is constant for $\vec{w}$ on the unit sphere regardless of $\sigma^2$, the term no longer depends on the secret matrix $\mat{C}$ and the attack will not work.

    \begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{hpp_parallelepiped_normal.png} % first figure itself
        \caption{Hidden parallelepiped problem in dimension 2 for normal distribution}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{hpp_normal_cube.png} % second figure itself
        \caption{Hidden hypercube problem in dimension 2 for normal distribution}
    \end{minipage}
\end{figure}
